{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import sys\nsys.argv = [\"\", \"--root\", \"/content/drive/MyDrive/Alzheimers/Data/Kaggle_MarcoPinamonti_Alzheimers_MRI\", \"--test_ratio\", \"0.2\", \"--seed\", \"42\", \"--mode\", \"copy\", \"--force\", \"0\"]\n\n\"\"\"\nPatient-level train/test split for the Marco dataset (ImageFolder-style).\n\n- Assumes data root has class subdirs like:\n    Kaggle_MarcoPinamonti_Alzheimers_MRI/\n      Mild_Impaired/\n      Moderate_Impaired/\n      No_Impairment/\n      Very_Mild_Impaired/\n\n- Filenames begin with a patient ID (leading integer), e.g.:\n    \"11 (6).jpg\", \"30.jpg\", \"3 (10).jpg\"\n\n- Splits **per class** at the **patient level** (no leakage), 80/20 by default.\n\nUsage:\n  python split_marco_patientwise.py \\\n    --root /content/drive/MyDrive/Alzheimers/Data/Kaggle_MarcoPinamonti_Alzheimers_MRI \\\n    --test_ratio 0.2 \\\n    --seed 42 \\\n    --mode copy \\\n    --force 0\n\"\"\"\n\nimport argparse\nimport random\nimport re\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nIMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n\ndef extract_patient_id(filename: str) -> str:\n    \"\"\"\n    Extract leading integer as patient ID.\n    Examples:\n      '11 (6).jpg' -> '11'\n      '30.jpg'     -> '30'\n    \"\"\"\n    name = filename.strip()\n    m = re.match(r\"^(\\d+)\\b\", name)\n    if not m:\n        raise ValueError(f\"Cannot parse patient id from filename: {filename!r}\")\n    return m.group(1)\n\ndef find_class_dirs(root: Path) -> List[Path]:\n    \"\"\"Return immediate subdirs that look like class folders (non-empty).\"\"\"\n    return [p for p in root.iterdir() if p.is_dir() and p.name.lower() not in {\"train\",\"test\"}]\n\ndef collect_by_patient(class_dir: Path) -> Dict[str, List[Path]]:\n    \"\"\"\n    For a class dir, return {patient_id: [list of image paths]}.\n    \"\"\"\n    buckets: Dict[str, List[Path]] = {}\n    for p in sorted(class_dir.glob(\"*\")):\n        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n            pid = extract_patient_id(p.name)\n            buckets.setdefault(pid, []).append(p)\n    return buckets\n\ndef split_patients(pids: List[str], test_ratio: float, seed: int) -> Tuple[List[str], List[str]]:\n    rng = random.Random(seed)\n    pids_shuffled = pids[:]\n    rng.shuffle(pids_shuffled)\n    n_total = len(pids_shuffled)\n    n_test = max(1, int(round(n_total * test_ratio))) if n_total > 1 else 1\n    test_ids = set(pids_shuffled[:n_test])\n    train_ids = [pid for pid in pids_shuffled if pid not in test_ids]\n    return train_ids, list(test_ids)\n\ndef safe_prepare_dir(d: Path, force: bool):\n    if d.exists():\n        # If directory exists and not empty, guard unless force\n        if any(d.iterdir()):\n            if not force:\n                raise SystemExit(f\"Refusing to write into non-empty dir: {d} (use --force 1 to clear)\")\n            shutil.rmtree(d)\n    d.mkdir(parents=True, exist_ok=True)\n\ndef transfer(paths: List[Path], dst_dir: Path, mode: str):\n    dst_dir.mkdir(parents=True, exist_ok=True)\n    for src in paths:\n        dst = dst_dir / src.name\n        if mode == \"copy\":\n            shutil.copy2(src, dst)\n        elif mode == \"move\":\n            shutil.move(str(src), str(dst))\n        elif mode == \"symlink\":\n            # relative symlink for portability\n            rel = Path(shutil.os.path.relpath(src, dst_dir))\n            if dst.exists():\n                dst.unlink()\n            dst.symlink_to(rel)\n        else:\n            raise ValueError(f\"Unknown mode: {mode}\")\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--root\", type=str, required=True, help=\"Marco dataset root (with class subfolders)\")\n    ap.add_argument(\"--test_ratio\", type=float, default=0.2, help=\"Fraction of patients per class for test\")\n    ap.add_argument(\"--seed\", type=int, default=42)\n    ap.add_argument(\"--mode\", type=str, default=\"copy\", choices=[\"copy\",\"move\",\"symlink\"],\n                    help=\"How to write files into train/test\")\n    ap.add_argument(\"--force\", type=int, default=0, help=\"If 1, clear existing train/test before writing\")\n    args = ap.parse_args()\n\n    root = Path(args.root).expanduser().resolve()\n    if not root.exists():\n        raise SystemExit(f\"Root not found: {root}\")\n\n    class_dirs = find_class_dirs(root)\n    if not class_dirs:\n        raise SystemExit(f\"No class subdirectories found under {root}\")\n\n    train_root = root / \"train\"\n    test_root  = root / \"test\"\n\n    # Prepare output dirs (guarded by --force)\n    safe_prepare_dir(train_root, force=bool(args.force))\n    safe_prepare_dir(test_root,  force=bool(args.force))\n\n    random.seed(args.seed)\n\n    summary = []\n    total_train_imgs = total_test_imgs = 0\n\n    for cdir in sorted(class_dirs, key=lambda p: p.name):\n        buckets = collect_by_patient(cdir)\n        if not buckets:\n            print(f\"[WARN] No images found in {cdir.name}; skipping.\")\n            continue\n\n        pids = sorted(buckets.keys(), key=lambda x: int(x))\n        train_ids, test_ids = split_patients(pids, test_ratio=args.test_ratio, seed=args.seed)\n\n        # Flatten files for each split\n        train_files = [p for pid in train_ids for p in buckets[pid]]\n        test_files  = [p for pid in test_ids  for p in buckets[pid]]\n\n        # Transfer\n        transfer(train_files, train_root / cdir.name, mode=args.mode)\n        transfer(test_files,  test_root  / cdir.name, mode=args.mode)\n\n        total_train_imgs += len(train_files)\n        total_test_imgs  += len(test_files)\n\n        summary.append({\n            \"class\": cdir.name,\n            \"n_patients\": len(pids),\n            \"train_patients\": len(train_ids),\n            \"test_patients\": len(test_ids),\n            \"train_imgs\": len(train_files),\n            \"test_imgs\": len(test_files),\n        })\n\n        print(f\"[{cdir.name}] patients: {len(pids)}  ->  train: {len(train_ids)} ({len(train_files)} imgs), \"\n              f\"test: {len(test_ids)} ({len(test_files)} imgs)\")\n\n    print(\"\\n=== Split complete ===\")\n    print(f\"Train images: {total_train_imgs}\")\n    print(f\"Test  images: {total_test_imgs}\")\n    print(f\"Train dir: {train_root}\")\n    print(f\"Test  dir: {test_root}\")\n\n    # Optional: write CSV summary\n    try:\n        import pandas as pd\n        import json\n        df = None\n        if summary:\n            df = pd.DataFrame(summary)\n            df.to_csv(root / \"split_summary.csv\", index=False)\n            (root / \"split_summary.json\").write_text(json.dumps(summary, indent=2))\n            print(f\"Summary saved to: {root/'split_summary.csv'}\")\n    except Exception:\n        pass\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
