{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n\"\"\"\nSummarize ArchSweep runs (and optional legacy hybrid runs) into a leaderboard + JSONs\nfor quick retraining and hybrid selection.\n\nOutputs (configurable):\n- <proj_root>/<leaderboard_dir>/leaderboard.csv         # ranked table\n- <proj_root>/<leaderboard_dir>/leaderboard.md          # markdown preview\n- <proj_root>/<leaderboard_dir>/top_per_arch.json       # best single config per architecture\n- <proj_root>/<leaderboard_dir>/top_for_hybrids.json    # shortlists per family (for hybrid pairing)\n- <proj_root>/<leaderboard_dir>/best_retrained.json     # arch -> {lr, dropout, label_smoothing, weight_decay, batch_size}\n\nNotes:\n- Use --leaderboard_dir to avoid overwriting your legacy leaderboard (default: Results/Model_Leaderboard).\n- Concatenation skips empty / all-NA DataFrames to avoid pandas FutureWarnings.\n\nUsage examples\n--------------\n# 1) Classic ArchSweep summary (legacy default output location)\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/ArchSweep \\\n  --families \"ResNet,DenseNet,Inception,ResNeXt,EffNet,MobileNetV2,MobileNetV3,VGG,CNN_Small,Other\" \\\n  --per_family 3 \\\n  --top_overall 100\n\n# 2) Singles-only summary to a SEPARATE leaderboard folder\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/Singles_Luke \\\n  --per_family 1 \\\n  --top_overall 100 \\\n  --leaderboard_dir Results/Model_Leaderboard_Singles\n\n# 3) Add more Singles sets into the same Singles-only leaderboard\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/Singles_Marco \\\n  --per_family 1 \\\n  --top_overall 100 \\\n  --leaderboard_dir Results/Model_Leaderboard_Singles\n\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/Singles_Falah \\\n  --per_family 1 \\\n  --top_overall 100 \\\n  --leaderboard_dir Results/Model_Leaderboard_Singles\n\n# 4) Mix Singles + legacy hybrid dirs (with optional HP JSONs) into a custom leaderboard dir\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/ArchSweep \\\n  --hybrid_dirs \"Results/Hybrid_Results_0826|hybrid_luke,Results/Hybrid_Results_0902|hybrid_luke+marco\" \\\n  --hybrid_hp \"hybrid_luke=/mnt/data/Hyperparameters_hybrid_for_Luke.json,hybrid_luke+marco=/mnt/data/Hyperparameters_hybrid_for_LukeMarco.json\" \\\n  --families \"ResNet,DenseNet,Inception,ResNeXt,EffNet,MobileNetV2,MobileNetV3,VGG,CNN_Small,Other\" \\\n  --per_family 3 \\\n  --top_overall 100 \\\n  --leaderboard_dir Results/Model_Leaderboard_Custom\n\"\"\"\n\nimport os, re, json, argparse\nfrom pathlib import Path\nimport pandas as pd\n\n# -------------------------\n# Family mapping\n# -------------------------\ndef family_of(arch: str) -> str:\n    a = (arch or \"\").lower()\n    if \"resnext\" in a: return \"ResNeXt\"\n    if \"resnet\"  in a: return \"ResNet\"\n    if \"densenet\" in a: return \"DenseNet\"\n    if \"inception\" in a: return \"Inception\"\n    if \"efficientnet\" in a or \"effnet\" in a: return \"EffNet\"\n    if \"mobilenetv3\" in a: return \"MobileNetV3\"\n    if \"mobilenetv2\" in a: return \"MobileNetV2\"\n    if \"vgg\" in a: return \"VGG\"\n    if \"cnn_small\" in a: return \"CNN_Small\"\n    return \"Other\"\n\n# -------------------------\n# ArchSweep parser\n# -------------------------\ndef parse_arch_sweep(sweep_dir: Path) -> pd.DataFrame:\n    csv_path = sweep_dir / \"arch_sweep_results.csv\"\n    if not csv_path.exists():\n        return pd.DataFrame()\n    df = pd.read_csv(csv_path)\n\n    # ensure columns exist\n    for col in [\"arch\",\"cfg_hash\",\"lr\",\"dropout\",\"weight_decay\",\"label_smoothing\",\n                \"batch_size\",\"best_val_acc\",\"best_epoch\",\"test_acc\",\"img_size\",\"run_dir\"]:\n        if col not in df.columns:\n            df[col] = None\n\n    # enrich with params.json (epochs, val_split, global_batch_size if present)\n    rows = []\n    for _, r in df.iterrows():\n        run_dir = Path(str(r[\"run_dir\"])) if pd.notna(r.get(\"run_dir\")) else None\n        params = {}\n        if run_dir and (run_dir / \"params.json\").exists():\n            try:\n                params = json.loads((run_dir / \"params.json\").read_text())\n            except Exception:\n                params = {}\n        row = dict(r)\n        row[\"epochs\"] = params.get(\"epochs\")\n        row[\"val_split\"] = params.get(\"val_split\")\n        row[\"global_batch_size\"] = params.get(\"global_batch_size\", row.get(\"batch_size\"))\n        row[\"family\"] = family_of(str(row[\"arch\"]))\n        row[\"source\"] = \"single_arch\"\n        rows.append(row)\n\n    out = pd.DataFrame(rows)\n    sort_cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in out.columns]\n    if sort_cols:\n        out = out.sort_values(sort_cols, ascending=False).reset_index(drop=True)\n    return out\n\n# -------------------------\n# Hybrid hyperparameter injection\n# -------------------------\ndef _std_hparams_from_json(p: Path) -> dict:\n    \"\"\"\n    Normalize keys from legacy hybrid JSONs to our leaderboard columns.\n\n    Accepts keys like:\n      - learning_rate OR initial_learning_rate OR lr\n      - dropout_rate\n      - batch_size\n      - epochs (optional)\n      - image_size (int or [H,W])\n      - validation_split\n      - num_workers (optional)\n      - weight_decay, label_smoothing (optional)\n\n    Returns a dict keyed by our canonical column names.\n    \"\"\"\n    try:\n        blob = json.loads(p.read_text())\n    except Exception:\n        return {}\n\n    def pick_lr(d):\n        for k in [\"learning_rate\", \"initial_learning_rate\", \"lr\"]:\n            if k in d and d[k] is not None:\n                try:\n                    return float(d[k])\n                except Exception:\n                    pass\n        return None\n\n    def pick_img_size(d):\n        v = d.get(\"image_size\")\n        if v is None:\n            return None\n        if isinstance(v, (list, tuple)) and len(v) > 0:\n            try:\n                return int(v[0])\n            except Exception:\n                return None\n        try:\n            return int(v)\n        except Exception:\n            return None\n\n    out = {\n        \"lr\": pick_lr(blob),\n        \"dropout\": float(blob[\"dropout_rate\"]) if \"dropout_rate\" in blob and blob[\"dropout_rate\"] is not None else None,\n        \"batch_size\": int(blob[\"batch_size\"]) if \"batch_size\" in blob and blob[\"batch_size\"] is not None else None,\n        \"epochs\": int(blob[\"epochs\"]) if \"epochs\" in blob and blob[\"epochs\"] is not None else None,\n        \"img_size\": pick_img_size(blob),\n        \"val_split\": float(blob[\"validation_split\"]) if \"validation_split\" in blob and blob[\"validation_split\"] is not None else None,\n        \"num_workers\": int(blob[\"num_workers\"]) if \"num_workers\" in blob and blob[\"num_workers\"] is not None else None,\n        \"weight_decay\": float(blob[\"weight_decay\"]) if \"weight_decay\" in blob and blob[\"weight_decay\"] is not None else None,\n        \"label_smoothing\": float(blob[\"label_smoothing\"]) if \"label_smoothing\" in blob and blob[\"label_smoothing\"] is not None else None,\n    }\n    return out\n\ndef parse_hybrid_hp_map(spec: str, proj_root: Path) -> dict:\n    \"\"\"\n    spec: 'label=/abs/or/rel.json,label2=/path2.json'\n    Paths are resolved under proj_root if relative.\n    Returns { label: normalized_hparams_dict }\n    \"\"\"\n    if not spec.strip():\n        return {}\n    out = {}\n    items = [s.strip() for s in spec.split(\",\") if s.strip()]\n    for it in items:\n        if \"=\" not in it:\n            continue\n        label, path_str = it.split(\"=\", 1)\n        label = label.strip()\n        p = Path(os.path.expanduser(path_str.strip()))\n        if not p.is_absolute():\n            p = (proj_root / p).resolve()\n        if p.exists():\n            out[label] = _std_hparams_from_json(p)\n        else:\n            out[label] = {}\n    return out\n\n# -------------------------\n# Hybrid results parser (summary.txt)\n# -------------------------\ndef parse_one_hybrid_dir(hybrid_dir: Path, label: str, hp_overrides_by_label: dict) -> pd.DataFrame:\n    \"\"\"Expect each subdir of hybrid_dir to have summary.txt with:\n         Test Acc: 0.9992\n         Precision: ...\n         Recall: ...\n         F1-score: ...\n    The subdir name is treated as the 'arch' (hybrid identifier).\n\n    We merge any provided hyperparameters for the given label into each row.\n    \"\"\"\n    rows = []\n    if not hybrid_dir.exists():\n        return pd.DataFrame()\n\n    # overrides for this label (may be empty)\n    hp = hp_overrides_by_label.get(label, {}) if label else {}\n\n    for model_dir in sorted(hybrid_dir.iterdir()):\n        if not model_dir.is_dir():\n            continue\n        p = model_dir / \"summary.txt\"\n        if not p.exists():\n            continue\n        text = p.read_text()\n        def grab(name):\n            m = re.search(rf\"{name}\\s*:\\s*([0-9.]+)\", text)\n            return float(m.group(1)) if m else None\n        row = {\n            \"arch\": model_dir.name,\n            \"source\": label or \"hybrid_legacy\",\n            \"test_acc\": grab(\"Test Acc\"),\n            \"precision\": grab(\"Precision\"),\n            \"recall\": grab(\"Recall\"),\n            \"f1\": grab(\"F1-score\"),\n            \"run_dir\": str(model_dir),\n            \"family\": \"Hybrid\",\n        }\n        # Merge in manual hyperparams (only if present)\n        for k, v in hp.items():\n            if v is not None:\n                if k == \"img_size\":\n                    row[\"img_size\"] = v\n                else:\n                    row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\ndef parse_hybrid_dirs(hybrid_specs: str, proj_root: Path, hp_overrides_by_label: dict) -> pd.DataFrame:\n    \"\"\"\n    Accepts a comma-separated list like:\n      \"Results/Hybrid_Results_0826|hybrid_luke,Results/Hybrid_Results_0902|hybrid_luke+marco\"\n    Each item may be \"path\" or \"path|label\".\n    Paths are resolved under proj_root if relative.\n    \"\"\"\n    if not hybrid_specs.strip():\n        return pd.DataFrame()\n    parts = [p.strip() for p in hybrid_specs.split(\",\") if p.strip()]\n    frames = []\n    for item in parts:\n        if \"|\" in item:\n            path_str, label = item.split(\"|\", 1)\n        else:\n            path_str, label = item, \"hybrid_legacy\"\n        p = Path(path_str)\n        if not p.is_absolute():\n            p = (proj_root / p).resolve()\n        frames.append(parse_one_hybrid_dir(p, label, hp_overrides_by_label))\n    if frames:\n        cleaned = [f for f in frames if not f.empty and not f.isna().all().all()]\n        return pd.concat(frames, ignore_index=True, sort=False)\n    return pd.DataFrame()\n\n# -------------------------\n# Helpers\n# -------------------------\ndef pick_best_per_arch(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        return df\n    cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in df.columns]\n    if cols:\n        sdf = df.sort_values(cols, ascending=False).copy()\n    else:\n        sdf = df.copy()\n    return sdf.drop_duplicates(subset=[\"arch\"], keep=\"first\").reset_index(drop=True)\n\ndef dataframe_to_markdown(df: pd.DataFrame, max_rows=30) -> str:\n    try:\n        return df.head(max_rows).to_markdown(index=False)\n    except Exception:\n        return df.head(max_rows).to_string(index=False)\n\n# -------------------------\n# Main\n# -------------------------\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True)\n    ap.add_argument(\"--sweep_dir\", type=str, default=\"Results/ArchSweep\")\n    ap.add_argument(\"--hybrid_dirs\", type=str, default=\"\",\n                    help=\"Comma list of PATH or PATH|LABEL, e.g., \"\n                         \"'Results/Hybrid_Results_0826|hybrid_luke,Results/Hybrid_Results_0902|hybrid_luke+marco'\")\n    ap.add_argument(\"--hybrid_hp\", type=str, default=\"\",\n                    help='Comma list of LABEL=JSONPATH providing hyperparameters for hybrid rows, '\n                         'e.g., \"hybrid_luke=/path/Luke.json,hybrid_luke+marco=/path/LukeMarco.json\"')\n    ap.add_argument(\"--families\", type=str, default=\"ResNet,DenseNet,Inception\",\n                    help=\"Comma-separated families to shortlist for hybrids (e.g., add ResNeXt,EffNet,MobileNetV2,MobileNetV3,VGG)\")\n    ap.add_argument(\"--per_family\", type=int, default=2, help=\"Top-N per family for hybrid shortlists\")\n    ap.add_argument(\"--top_overall\", type=int, default=50, help=\"Rows to keep in leaderboard display\")\n    ap.add_argument(\"--leaderboard_dir\", type=str, default=\"Results/Model_Leaderboard\", \n                    help=\"Output directory for leaderboard files (CSV/MD and JSONs). \"\n                         \"Default keeps legacy path: Results/Model_Leaderboard\"\n)\n    args = ap.parse_args()\n\n    proj_root = Path(os.path.expanduser(args.proj_root))\n    sweep_dir = proj_root / args.sweep_dir\n    out_dir = (proj_root / args.leaderboard_dir).resolve()\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # parse hyperparam overrides for hybrid labels\n    hp_overrides_by_label = parse_hybrid_hp_map(args.hybrid_hp, proj_root)\n\n    singles = parse_arch_sweep(sweep_dir)\n    hybrids = parse_hybrid_dirs(args.hybrid_dirs, proj_root, hp_overrides_by_label)\n\n    frames = [df for df in [singles, hybrids] if not df.empty and not df.isna().all().all()]\n    all_df = pd.concat([singles, hybrids], ignore_index=True, sort=False)\n\n    # Name for display\n    if \"cfg_hash\" in all_df.columns:\n        def mkname(r):\n            if r.get(\"source\") == \"single_arch\":\n                return f\"{r.get('arch','?')} ({str(r.get('cfg_hash',''))[:6]})\"\n            return f\"{r.get('arch','?')} [{r.get('source','hybrid')}]\"\n        all_df[\"name\"] = all_df.apply(mkname, axis=1)\n    else:\n        all_df[\"name\"] = all_df.get(\"arch\", \"?\")\n\n    # Sort by test_acc desc then best_val_acc\n    sort_cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in all_df.columns]\n    if sort_cols:\n        all_df = all_df.sort_values(sort_cols, ascending=False).reset_index(drop=True)\n\n    # Leaderboard view\n    leaderboard = all_df.copy()\n    if args.top_overall and args.top_overall > 0:\n        leaderboard = leaderboard.head(args.top_overall).copy()\n\n    # also keep any per-dataset accuracy columns coming from 3-1\n    extra_acc_cols = [c for c in leaderboard.columns if c.startswith(\"acc_\")]\n\n    keep_cols = [c for c in [\n        \"name\",\"source\",\"family\",\"arch\",\"img_size\",\"batch_size\",\"global_batch_size\",\n        \"lr\",\"dropout\",\"weight_decay\",\"label_smoothing\",\n        \"best_val_acc\",\"best_epoch\",\"test_acc\",\"precision\",\"recall\",\"f1\",\"run_dir\",\"cfg_hash\",\"epochs\",\"val_split\",\n        \"num_workers\"\n    ] if c in leaderboard.columns] + extra_acc_cols\n    leaderboard = leaderboard[keep_cols]\n\n    # Save leaderboard\n    leaderboard_csv = out_dir / \"leaderboard.csv\"\n    leaderboard_md  = out_dir / \"leaderboard.md\"\n    leaderboard.to_csv(leaderboard_csv, index=False)\n    leaderboard_md.write_text(dataframe_to_markdown(leaderboard))\n\n    # Best per arch (singles only)\n    singles_only = all_df[all_df[\"source\"]==\"single_arch\"].copy() if \"source\" in all_df.columns else all_df.copy()\n    best_per_arch = pick_best_per_arch(singles_only)\n\n    top_per_arch_json = {\n        row[\"arch\"]: {\n            \"cfg_hash\": row.get(\"cfg_hash\"),\n            \"run_dir\": row.get(\"run_dir\"),\n            \"lr\": float(row.get(\"lr\")) if pd.notna(row.get(\"lr\")) else None,\n            \"dropout\": float(row.get(\"dropout\")) if pd.notna(row.get(\"dropout\")) else None,\n            \"label_smoothing\": float(row.get(\"label_smoothing\")) if pd.notna(row.get(\"label_smoothing\")) else None,\n            \"weight_decay\": float(row.get(\"weight_decay\")) if pd.notna(row.get(\"weight_decay\")) else None,\n            # prefer global_batch_size if present (global semantics in your trainer)\n            \"batch_size\": int(row.get(\"global_batch_size\") if pd.notna(row.get(\"global_batch_size\")) else row.get(\"batch_size\")) if pd.notna(row.get(\"batch_size\")) or pd.notna(row.get(\"global_batch_size\")) else None,\n            \"best_val_acc\": float(row.get(\"best_val_acc\")) if pd.notna(row.get(\"best_val_acc\")) else None,\n            \"test_acc\": float(row.get(\"test_acc\")) if pd.notna(row.get(\"test_acc\")) else None,\n        }\n        for _, row in best_per_arch.iterrows()\n    }\n    (out_dir / \"top_per_arch.json\").write_text(json.dumps(top_per_arch_json, indent=2))\n\n    # Shortlists per family to build hybrids\n    wanted_families = [f.strip() for f in args.families.split(\",\") if f.strip()]\n    shortlists = {}\n    for fam in wanted_families:\n        fam_rows = singles_only[singles_only[\"family\"] == fam].copy()\n        if fam_rows.empty:\n            continue\n        fam_rows = fam_rows.sort_values(sort_cols, ascending=False).head(args.per_family)\n        shortlists[fam] = [\n            {\n                \"arch\": r[\"arch\"],\n                \"cfg_hash\": r.get(\"cfg_hash\"),\n                \"run_dir\": r.get(\"run_dir\"),\n                \"lr\": float(r.get(\"lr\")) if pd.notna(r.get(\"lr\")) else None,\n                \"dropout\": float(r.get(\"dropout\")) if pd.notna(r.get(\"dropout\")) else None,\n                \"label_smoothing\": float(r.get(\"label_smoothing\")) if pd.notna(r.get(\"label_smoothing\")) else None,\n                \"weight_decay\": float(r.get(\"weight_decay\")) if pd.notna(r.get(\"weight_decay\")) else None,\n                \"batch_size\": int(r.get(\"global_batch_size\") if pd.notna(r.get(\"global_batch_size\")) else r.get(\"batch_size\")) if pd.notna(r.get(\"batch_size\")) or pd.notna(r.get(\"global_batch_size\")) else None,\n                \"best_val_acc\": float(r.get(\"best_val_acc\")) if pd.notna(r.get(\"best_val_acc\")) else None,\n                \"test_acc\": float(r.get(\"test_acc\")) if pd.notna(r.get(\"test_acc\")) else None,\n            }\n            for _, r in fam_rows.iterrows()\n        ]\n    (out_dir / \"top_for_hybrids.json\").write_text(json.dumps(shortlists, indent=2))\n\n    # Emit a best_retrained.json (schema your trainer expects)\n    best_retrained = {}\n    for arch, blob in top_per_arch_json.items():\n        best_retrained[arch] = {\n            \"lr\": blob.get(\"lr\"),\n            \"label_smoothing\": blob.get(\"label_smoothing\"),\n            \"dropout\": blob.get(\"dropout\"),\n            \"weight_decay\": blob.get(\"weight_decay\"),\n            \"batch_size\": blob.get(\"batch_size\"),\n        }\n    (out_dir / \"best_retrained.json\").write_text(json.dumps(best_retrained, indent=2))\n\n    print(\"Saved:\")\n    print(f\"- {leaderboard_csv}\")\n    print(f\"- {leaderboard_md}\")\n    print(f\"- {out_dir / 'top_per_arch.json'}\")\n    print(f\"- {out_dir / 'top_for_hybrids.json'}\")\n    print(f\"- {out_dir / 'best_retrained.json'}\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
