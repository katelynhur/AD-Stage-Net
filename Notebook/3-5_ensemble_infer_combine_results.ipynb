{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n\"\"\"\nCombine multiple ensemble_results.csv (from different test datasets) into one table.\n\n- Reads N inputs specified as:  --inputs \"path1::Luke,path2::Marco,path3::Falah\"\n  (You may also use '|' instead of '::' if thatâ€™s what your wrapper has.)\n- For each file, takes the BEST row per unique ensemble combo (by acc).\n- Builds a canonical combo_key from sorted members_run_dirs (fallback to members_names).\n- Produces per-dataset accuracy columns: acc_<LABEL>\n- Adds aggregate columns: avg_acc, min_acc, std_acc\n- Keeps metadata (members_names, members_families, members_run_dirs, n_models) from\n  the first time a combo appears.\n\nOutput:\n  <out_dir>/combined_ensemble_results.csv\n  <out_dir>/combined_ensemble_results.md   (top 50 pretty table)\n\"\"\"\n\nimport argparse\nfrom pathlib import Path\nimport os\nimport math\nimport pandas as pd\nimport numpy as np\n\n\ndef parse_inputs(spec: str):\n    \"\"\"\n    Accepts comma-separated items: 'path::Label' or 'path|Label'.\n    If no label provided, infer from parent folder name or file stem.\n    Returns list of (Path, label).\n    \"\"\"\n    items = []\n    if not spec.strip():\n        return items\n    for raw in [p.strip() for p in spec.split(\",\") if p.strip()]:\n        if \"::\" in raw:\n            pth, lab = raw.split(\"::\", 1)\n        elif \"|\" in raw:\n            pth, lab = raw.split(\"|\", 1)\n        else:\n            pth = raw\n            lab = Path(pth).parent.name or Path(pth).stem\n        items.append((Path(os.path.expanduser(pth)).resolve(), lab.strip()))\n    return items\n\n\ndef make_combo_key(row: pd.Series) -> str:\n    \"\"\"\n    Canonical key built from sorted members_run_dirs (preferred).\n    Fallback to sorted members_names if needed.\n    \"\"\"\n    if \"members_run_dirs\" in row and pd.notna(row[\"members_run_dirs\"]):\n        parts = [p.strip() for p in str(row[\"members_run_dirs\"]).split(\"|\")]\n        parts = [p for p in parts if p]\n        parts = sorted(parts)\n        return \"||\".join(parts)\n    # fallback\n    names = [p.strip() for p in str(row.get(\"members_names\", \"\")).split(\"+\")]\n    names = [p for p in names if p]\n    names = sorted(names)\n    return \" + \".join(names)\n\n\ndef best_per_combo(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    From an ensemble_results.csv, keep only the best row per combo_key by 'acc'.\n    \"\"\"\n    if \"acc\" not in df.columns:\n        raise SystemExit(\"Input CSV missing required column 'acc'.\")\n\n    df = df.copy()\n    df[\"combo_key\"] = df.apply(make_combo_key, axis=1)\n    # sort by acc desc and drop dup combo_key\n    df = df.sort_values(\"acc\", ascending=False).drop_duplicates(subset=[\"combo_key\"], keep=\"first\")\n    return df\n\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--inputs\", type=str, required=True,\n                    help='Comma list like \"path1::Luke,path2::Marco,path3::Falah\" (also supports \"|\" instead of \"::\").')\n    ap.add_argument(\"--out_dir\", type=str, required=True)\n    ap.add_argument(\"--top_preview\", type=int, default=50, help=\"Rows to show in markdown preview\")\n    args = ap.parse_args()\n\n    out_dir = Path(os.path.expanduser(args.out_dir)).resolve()\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    inputs = parse_inputs(args.inputs)\n    if not inputs:\n        raise SystemExit(\"No inputs provided to --inputs\")\n\n    # Accumulate rows keyed by combo_key\n    store = {}  # combo_key -> dict of merged fields\n\n    for csv_path, label in inputs:\n        if not csv_path.exists():\n            raise SystemExit(f\"Missing input file: {csv_path}\")\n        df = pd.read_csv(csv_path)\n\n        # Normalize and keep best per combo\n        df_best = best_per_combo(df)\n\n        # Walk rows and merge\n        for _, r in df_best.iterrows():\n            key = r[\"combo_key\"]\n            if key not in store:\n                # capture metadata from the first dataset that has this combo\n                store[key] = {\n                    \"combo_key\": key,\n                    \"members_names\": r.get(\"members_names\"),\n                    \"members_families\": r.get(\"members_families\"),\n                    \"members_run_dirs\": r.get(\"members_run_dirs\"),\n                    \"n_models\": int(r.get(\"n_models\")) if pd.notna(r.get(\"n_models\")) else None,\n                }\n            # per-dataset accuracy column\n            store[key][f\"acc_{label}\"] = float(r[\"acc\"]) if pd.notna(r[\"acc\"]) else np.nan\n\n    # Convert to DataFrame\n    rows = list(store.values())\n    combined = pd.DataFrame(rows)\n\n    # Compute aggregates over all acc_* columns present\n    acc_cols = [c for c in combined.columns if c.startswith(\"acc_\")]\n    if acc_cols:\n        combined[\"avg_acc\"] = combined[acc_cols].mean(axis=1, skipna=True)\n        combined[\"min_acc\"] = combined[acc_cols].min(axis=1, skipna=True)\n        combined[\"std_acc\"] = combined[acc_cols].std(axis=1, ddof=0, skipna=True)\n    else:\n        combined[\"avg_acc\"] = np.nan\n        combined[\"min_acc\"] = np.nan\n        combined[\"std_acc\"] = np.nan\n\n    # Sort by robust criteria\n    combined = combined.sort_values([\"min_acc\", \"avg_acc\", \"std_acc\"], ascending=[False, False, True]).reset_index(drop=True)\n\n    # Save\n    out_csv = out_dir / \"combined_ensemble_results.csv\"\n    combined.to_csv(out_csv, index=False)\n\n    # Markdown preview\n    out_md = out_dir / \"combined_ensemble_results.md\"\n    head = combined.head(args.top_preview).copy()\n    try:\n        out_md.write_text(head.to_markdown(index=False))\n    except Exception:\n        out_md.write_text(head.to_string(index=False))\n\n    print(\"Wrote:\")\n    print(\"-\", out_csv)\n    print(\"-\", out_md)\n    print(f\"Rows: {len(combined)} | Datasets merged: {len(acc_cols)} [{', '.join(acc_cols)}]\")\n\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
