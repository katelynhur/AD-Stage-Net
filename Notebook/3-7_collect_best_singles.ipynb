{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n\"\"\"\nExport best single-model checkpoints (best.pt only) per architecture.\n\n- Reads <proj_root>/<sweep_dir>/arch_sweep_results.csv\n- Picks the top row per arch by test_acc (fallback: best_val_acc)\n- Copies <run_dir>/best.pt -> <out_dir>/<ARCH>_best.pt  (optionally add cfg_hash)\n- Writes a small manifest CSV/JSON for traceability.\n\nUsage:\n  python 3-6_export_best_ckpts.py \\\n    --proj_root /content/drive/MyDrive/Alzheimers \\\n    --sweep_dir Results/ArchSweep \\\n    --out_dir Results/BestSingles \\\n    --include_hash 0 \\\n    --overwrite 1\n\"\"\"\n\nimport argparse\nimport json\nimport shutil\nimport sys\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport time\n\ndef pick_best_row(df_arch: pd.DataFrame) -> pd.Series:\n    \"\"\"Pick best by test_acc (desc), then best_val_acc (desc).\"\"\"\n    df = df_arch.copy()\n    for col in (\"test_acc\", \"best_val_acc\"):\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n        else:\n            df[col] = np.nan\n    df[\"_rank_key\"] = list(zip(\n        df[\"test_acc\"].fillna(-np.inf),\n        df[\"best_val_acc\"].fillna(-np.inf),\n    ))\n    idx = df[\"_rank_key\"].idxmax()\n    return df.loc[idx].drop(labels=[\"_rank_key\"])\n\ndef safe_arch_name(s: str) -> str:\n    return \"\".join(ch if ch.isalnum() or ch in (\"-\", \"_\") else \"_\" for ch in s.strip())\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True)\n    ap.add_argument(\"--sweep_dir\", type=str, default=\"Results/ArchSweep\")\n    ap.add_argument(\"--out_dir\", type=str, default=\"Results/BestSingles\")\n    ap.add_argument(\"--include_hash\", type=int, default=0, help=\"Append _<cfg_hash> to filename\")\n    ap.add_argument(\"--overwrite\", type=int, default=0, help=\"Overwrite existing exported files\")\n    args = ap.parse_args()\n\n    proj_root = Path(args.proj_root).expanduser().resolve()\n    sweep_dir = (proj_root / args.sweep_dir).resolve()\n    out_dir = (proj_root / args.out_dir).resolve()\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    csv_path = sweep_dir / \"arch_sweep_results.csv\"\n    if not csv_path.exists():\n        print(f\"[ERR] Missing CSV: {csv_path}\", file=sys.stderr)\n        sys.exit(1)\n\n    df = pd.read_csv(csv_path)\n    needed = {\"arch\", \"run_dir\", \"cfg_hash\"}\n    if not needed.issubset(df.columns):\n        print(f\"[ERR] CSV missing required columns: {needed - set(df.columns)}\", file=sys.stderr)\n        sys.exit(1)\n\n    # Normalize run_dir to absolute paths\n    def norm_run_dir(p):\n        if pd.isna(p): return None\n        pth = Path(str(p))\n        return pth if pth.is_absolute() else (proj_root / pth).resolve()\n    df[\"run_dir\"] = df[\"run_dir\"].apply(norm_run_dir)\n\n    # Select best per arch\n    best_rows = []\n    for arch, g in df.groupby(\"arch\", sort=False):\n        try:\n            row = pick_best_row(g)\n        except Exception:\n            row = g.iloc[0]\n        best_rows.append(row)\n    sel = pd.DataFrame(best_rows).reset_index(drop=True)\n\n    # Export\n    manifest_rows = []\n    exported = 0\n    skipped = 0\n    for _, r in sel.iterrows():\n        arch = str(r[\"arch\"])\n        run_dir: Path = r[\"run_dir\"]\n        cfg_hash = str(r.get(\"cfg_hash\") or \"\")\n        if run_dir is None or not run_dir.exists():\n            print(f\"[WARN] Missing run_dir for {arch}: {r.get('run_dir')}\")\n            skipped += 1\n            continue\n        src = run_dir / \"best.pt\"\n        if not src.exists():\n            print(f\"[WARN] best.pt not found for {arch}: {src}\")\n            skipped += 1\n            continue\n\n        base = f\"{safe_arch_name(arch)}_best\"\n        if args.include_hash and cfg_hash:\n            base = f\"{base}_{cfg_hash[:10]}\"\n        dst = out_dir / f\"{base}.pt\"\n\n        if dst.exists() and not args.overwrite:\n            print(f\"[SKIP] Exists: {dst}\")\n        else:\n            if dst.exists():\n                dst.unlink()\n            shutil.copy2(src, dst)\n            print(f\"[COPIED] {arch}: {src} -> {dst}\")\n            exported += 1\n\n        manifest_rows.append({\n            \"arch\": arch,\n            \"cfg_hash\": cfg_hash,\n            \"source_run_dir\": str(run_dir),\n            \"src_best\": str(src),\n            \"dst_file\": str(dst),\n            \"test_acc\": float(r[\"test_acc\"]) if \"test_acc\" in r and pd.notna(r[\"test_acc\"]) else None,\n            \"best_val_acc\": float(r[\"best_val_acc\"]) if \"best_val_acc\" in r and pd.notna(r[\"best_val_acc\"]) else None,\n            \"img_size\": int(r[\"img_size\"]) if \"img_size\" in r and pd.notna(r[\"img_size\"]) else None,\n        })\n\n    # Save manifest\n    manifest_df = pd.DataFrame(manifest_rows).sort_values(\"arch\").reset_index(drop=True)\n    (out_dir / \"export_manifest.csv\").write_text(manifest_df.to_csv(index=False))\n    (out_dir / \"export_manifest.json\").write_text(json.dumps(manifest_rows, indent=2))\n    (out_dir / \"README.txt\").write_text(\n        f\"BestSingles Checkpoints\\n\"\n        f\"=======================\\n\"\n        f\"Created: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        f\"Source CSV: {csv_path}\\n\"\n        f\"Files: one <ARCH>_best(.<hash>).pt per architecture.\\n\"\n    )\n\n    print(f\"\\nDone. Exported: {exported} | Skipped: {skipped}\")\n    print(f\"- {out_dir/'export_manifest.csv'}\")\n    print(f\"- {out_dir/'export_manifest.json'}\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
