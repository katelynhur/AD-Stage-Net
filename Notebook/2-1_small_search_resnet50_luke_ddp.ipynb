{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n\"\"\"\nParallel hyperparameter search for ResNet50 on Luke Chugh dataset.\n\nUpdates in this version:\n- Robust CSV IO:\n    * safe_read_results_csv() repairs malformed rows (extra/fewer commas).\n    * Rank 0 reads prior CSV and broadcasts 'seen' param keys to other ranks.\n    * Final write uses explicit quoting/escaping.\n- DDP hygiene:\n    * Set CUDA device BEFORE init_process_group to avoid warnings.\n    * Always destroy_process_group() via try/finally.\n\nOther features (as before):\n- Self-launch with torchrun on A800s (physical 0,2,3) for 3 procs.\n- Trial-level parallelism (no DDP wrapper for the model).\n- Stratified 80/20 validation split; AdamW + ReduceLROnPlateau + early stopping.\n- Label smoothing + class weights; Pad->Resize(224) transforms.\n- Checkpoints saved as ckpt_<paramkey>.pt; skip trials already done (CSV or ckpt).\n- Rank 0 aggregates CSV and copies best checkpoint to best_resnet50.pt.\n\nRun:\n  python small_search_resnet50_luke_ddp.py\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport json\nimport random\nimport shutil\nimport csv\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.distributed as dist\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import functional as F, InterpolationMode\n\n\n# -----------------------------------------------------------------------------\n# Self-launch with torchrun on selected physical GPUs (0,2,3)\n# -----------------------------------------------------------------------------\ndef self_launch_if_needed():\n    if \"LOCAL_RANK\" in os.environ:\n        return  # already under torchrun\n\n    gpu_list = \"0,2,3\"  # A800s only (skip T1000 at index 1)\n    nproc = len(gpu_list.split(\",\"))\n\n    env = os.environ.copy()\n    env[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    env[\"CUDA_VISIBLE_DEVICES\"] = gpu_list\n\n    torchrun = shutil.which(\"torchrun\")\n    if torchrun:\n        cmd = [torchrun, f\"--nproc_per_node={nproc}\", sys.argv[0], \"--_launched=1\"]\n    else:\n        cmd = [sys.executable, \"-m\", \"torch.distributed.run\", f\"--nproc_per_node={nproc}\", sys.argv[0], \"--_launched=1\"]\n\n    for a in sys.argv[1:]:\n        if a != \"--_launched=1\":\n            cmd.append(a)\n\n    print(\"Launching:\", \" \".join(cmd))\n    os.execvpe(cmd[0], cmd, env)\n\n\nself_launch_if_needed()\n\n\n# -----------------------------------------------------------------------------\n# Utilities: robust CSV and broadcast helpers\n# -----------------------------------------------------------------------------\ndef safe_read_results_csv(csv_path: Path) -> pd.DataFrame:\n    \"\"\"\n    Read results CSV even if some rows are malformed (too many/few commas).\n    Repairs by merging extra columns into the last column or padding missing ones.\n    \"\"\"\n    if not csv_path.exists() or csv_path.stat().st_size == 0:\n        return pd.DataFrame()\n\n        reader = csv.reader(f)\n        rows = list(reader)\n\n    if not rows:\n        return pd.DataFrame()\n\n    header = rows[0]\n    n = len(header)\n    fixed_rows = []\n    bad = 0\n\n    for r in rows[1:]:\n        if len(r) == n:\n            fixed_rows.append(r)\n        elif len(r) > n:\n            merged = r[:n-1] + [\",\".join(r[n-1:])]\n            fixed_rows.append(merged)\n            bad += 1\n        else:  # len(r) < n\n            fixed_rows.append(r + [\"\"] * (n - len(r)))\n            bad += 1\n\n    if bad:\n        print(f\"[safe_read_results_csv] Repaired {bad} malformed rows in {csv_path.name}\")\n\n    return pd.DataFrame(fixed_rows, columns=header)\n\n\ndef broadcast_object(obj, src=0):\n    \"\"\"Broadcast a picklable Python object from src to all ranks.\"\"\"\n    rank = int(os.environ.get(\"RANK\", \"0\"))\n    container = [obj if rank == src else None]\n    dist.broadcast_object_list(container, src=src)\n    return container[0]\n\n\n# -----------------------------------------------------------------------------\n# Distributed setup\n# -----------------------------------------------------------------------------\ndef ddp_setup():\n    # Set device FIRST to silence the device-id warning\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    if torch.cuda.is_available():\n        torch.cuda.set_device(local_rank)\n    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n    rank = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    return rank, world_size, local_rank\n\n\n# -----------------------------------------------------------------------------\n# Transforms (Pad -> Resize 224)\n# -----------------------------------------------------------------------------\nclass PadToSquare:\n    def __call__(self, img):\n        w, h = F.get_image_size(img)\n        s = max(w, h)\n        pad_l = (s - w) // 2\n        pad_r = s - w - pad_l\n        pad_t = (s - h) // 2\n        pad_b = s - h - pad_t\n        return F.pad(img, [pad_l, pad_t, pad_r, pad_b], fill=0)\n\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD  = [0.229, 0.224, 0.225]\n\ntf_train = transforms.Compose([\n    PadToSquare(),\n    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC, antialias=True),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomRotation(10, interpolation=InterpolationMode.BILINEAR),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n])\n\ntf_eval = transforms.Compose([\n    PadToSquare(),\n    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC, antialias=True),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n])\n\n\n# -----------------------------------------------------------------------------\n# Param key helpers (for skipping & checkpoint naming)\n# -----------------------------------------------------------------------------\nPARAM_KEYS = [\"lr\", \"weight_decay\", \"dropout\", \"label_smoothing\", \"batch_size\"]\n\ndef param_key(p: dict) -> str:\n    # stable compact key for filenames (avoid scientific notation)\n    def fmt(x):\n        if isinstance(x, float):\n            return f\"{x:.8f}\".rstrip(\"0\").rstrip(\".\")\n        return str(x)\n    return \"__\".join([f\"{k}-{fmt(p[k])}\" for k in PARAM_KEYS])\n\ndef ckpt_path_for(p: dict, out_dir: Path) -> Path:\n    return out_dir / f\"ckpt_{param_key(p)}.pt\"\n\n\n# -----------------------------------------------------------------------------\n# Model builder (ResNet50 + optional dropout)\n# -----------------------------------------------------------------------------\ndef build_resnet50(num_classes: int, dropout: float):\n    m = models.resnet50(weights=\"IMAGENET1K_V1\")\n    in_dim = m.fc.in_features\n    if dropout and float(dropout) > 0:\n        m.fc = nn.Sequential(nn.Dropout(p=float(dropout)), nn.Linear(in_dim, num_classes))\n    else:\n        m.fc = nn.Linear(in_dim, num_classes)\n    return m\n\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval()\n    correct = total = 0\n    for xb, yb in loader:\n        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n        pred = model(xb).argmax(1)\n        correct += (pred == yb).sum().item()\n        total += yb.size(0)\n    return correct / total if total else 0.0\n\n\ndef run_trial(params, trial_id: int, loaders, num_classes, class_weights, device, out_dir: Path):\n    lr  = float(params[\"lr\"])\n    wd  = float(params[\"weight_decay\"])\n    dp  = float(params[\"dropout\"])\n    ls  = float(params[\"label_smoothing\"])\n    bs  = int(params[\"batch_size\"])\n\n    train_loader, val_loader, test_loader = loaders\n\n    model = build_resnet50(num_classes, dp).to(device)\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=ls)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6\n    )\n\n    best_val, best_epoch = -1.0, -1\n    patience, wait = 7, 0\n    max_epochs = 12\n\n    ckpt_tmp = ckpt_path_for(params, out_dir)\n\n    for epoch in range(max_epochs):\n        model.train()\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n            optimizer.zero_grad(set_to_none=True)\n            loss = criterion(model(xb), yb)\n            loss.backward()\n            optimizer.step()\n\n        val_acc = evaluate(model, val_loader, device)\n        scheduler.step(val_acc)\n\n        if val_acc > best_val + 1e-6:\n            best_val, best_epoch = val_acc, epoch + 1\n            wait = 0\n            torch.save(model.state_dict(), ckpt_tmp)\n        else:\n            wait += 1\n            if wait >= patience:\n                break\n\n    # Test best\n    model.load_state_dict(torch.load(ckpt_tmp, map_location=device))\n    test_acc = evaluate(model, test_loader, device)\n\n    return {\n        \"rank\": int(os.environ.get(\"RANK\", \"0\")),\n        \"trial\": trial_id,\n        \"lr\": lr,\n        \"weight_decay\": wd,\n        \"dropout\": dp,\n        \"label_smoothing\": ls,\n        \"batch_size\": bs,\n        \"best_val_acc\": round(best_val, 6),\n        \"best_epoch\": best_epoch,\n        \"test_acc\": round(test_acc, 6),\n        \"ckpt_path\": str(ckpt_tmp),\n    }\n\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\ndef main():\n    rank, world_size, local_rank = ddp_setup()\n    device = torch.device(f\"cuda:{local_rank}\" if torch.cuda.is_available() else \"cpu\")\n    is_main = rank == 0\n\n    print(f\"[rank {rank}] LOCAL_RANK={local_rank}, WORLD_SIZE={world_size}\")\n    print(f\"[rank {rank}] CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n    if torch.cuda.is_available():\n        print(f\"[rank {rank}] torch sees {torch.cuda.device_count()} device(s):\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"  cuda:{i} -> {torch.cuda.get_device_name(i)}\")\n\n    # Reproducibility\n    SEED = 42 + rank\n    random.seed(SEED); np.random.seed(SEED)\n    torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Paths\n    PROJ_ROOT = Path(\"/content/drive/MyDrive/Alzheimers\")\n    DATA_ROOT = PROJ_ROOT / \"Data\" / \"Kaggle_LukeChugh_Best_Alzheimers_MRI\"\n    TRAIN_DIR = DATA_ROOT / \"train\"\n    TEST_DIR  = DATA_ROOT / \"test\"\n\n    OUT_DIR   = PROJ_ROOT / \"Results\" / \"HP_Search_Luke_DDP\"\n    OUT_DIR.mkdir(parents=True, exist_ok=True)\n    CSV_PATH  = OUT_DIR / \"resnet50_luke_small_search_ddp.csv\"\n    if is_main:\n        print(f\"[rank 0] Results dir: {OUT_DIR}\")\n\n    # Datasets & stratified 80/20 val split\n    full_train_aug  = datasets.ImageFolder(str(TRAIN_DIR), transform=tf_train)\n    full_train_eval = datasets.ImageFolder(str(TRAIN_DIR), transform=tf_eval)\n    test_set        = datasets.ImageFolder(str(TEST_DIR),  transform=tf_eval)\n\n    num_classes = len(full_train_aug.classes)\n    class_names = full_train_aug.classes\n    if is_main:\n        print(\"Classes:\", class_names)\n\n    targets = [y for _, y in full_train_aug.samples]\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n    train_ds = Subset(full_train_aug, train_idx)\n    val_ds   = Subset(full_train_eval, val_idx)\n\n    # Class weights from TRAIN subset\n    train_labels = [full_train_aug.samples[i][1] for i in train_idx]\n    class_counts = np.bincount(train_labels, minlength=num_classes)\n    total = class_counts.sum()\n    weights = total / (class_counts + 1e-12)\n    class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n    if is_main:\n        print(\"Class counts (train):\", class_counts.tolist())\n\n    def make_loaders(bs: int):\n        common = dict(batch_size=bs, num_workers=4, pin_memory=True, persistent_workers=True)\n        return (\n            DataLoader(train_ds, shuffle=True,  **common),\n            DataLoader(val_ds,   shuffle=False, **common),\n            DataLoader(test_set, shuffle=False, **common),\n        )\n\n    # Search grid\n    LRS = [5e-5, 1e-4, 2e-4]\n    WDS = [5e-5, 1e-4, 2e-4]\n    DPS = [0.2, 0.3, 0.5]\n    LSS = [0.03, 0.05, 0.07]\n    BSS = [16, 32]\n\n    grid = [\n        {\"lr\": lr, \"weight_decay\": wd, \"dropout\": dp, \"label_smoothing\": ls, \"batch_size\": bs}\n        for lr in LRS\n        for wd in WDS\n        for dp in DPS\n        for ls in LSS\n        for bs in BSS\n    ]\n\n    # Build 'seen' from existing CSV on rank 0, then broadcast\n    seen_from_csv = set()\n    if is_main and CSV_PATH.exists():\n        try:\n            df_prev = safe_read_results_csv(CSV_PATH)\n            if not df_prev.empty:\n                # Ensure expected columns exist\n                missing = [c for c in [\"lr\",\"weight_decay\",\"dropout\",\"label_smoothing\",\"batch_size\"] if c not in df_prev.columns]\n                if missing:\n                    print(f\"[rank 0] Warning: CSV missing columns {missing}; proceeding with whatever is present.\")\n                for _, r in df_prev.iterrows():\n                    try:\n                        p = {\n                            \"lr\": float(r[\"lr\"]),\n                            \"weight_decay\": float(r[\"weight_decay\"]),\n                            \"dropout\": float(r[\"dropout\"]),\n                            \"label_smoothing\": float(r[\"label_smoothing\"]),\n                            \"batch_size\": int(float(r[\"batch_size\"]))  # handle e.g., \"32.0\"\n                        }\n                        seen_from_csv.add(param_key(p))\n                    except Exception:\n                        # Skip rows that don't parse cleanly\n                        continue\n        except Exception as e:\n            print(f\"[rank 0] Warning: could not parse existing CSV for skipping: {e}\")\n\n    seen_from_csv = broadcast_object(seen_from_csv, src=0)\n\n    # Also mark completed if a param-keyed checkpoint exists\n    seen = set(seen_from_csv)\n    for p in grid:\n        if ckpt_path_for(p, OUT_DIR).exists():\n            seen.add(param_key(p))\n\n    # Filter grid: only run trials not seen\n    grid_to_run = [p for p in grid if param_key(p) not in seen]\n\n    # Shard across ranks\n    my_grid = grid_to_run[rank::world_size]\n    if is_main:\n        print(f\"Total trials: {len(grid)} | already done/seen: {len(seen)} | remaining: {len(grid_to_run)} | per-rank: ~{len(my_grid)}\")\n\n    # Run local trials\n    TRIALS_LOCAL = []\n    start = time.time()\n    for t_id, params in enumerate(my_grid, 1):\n        bs = int(params[\"batch_size\"])\n        loaders = make_loaders(bs)\n        print(f\"[rank {rank}] Trial {t_id}/{len(my_grid)} :: {params}\")\n        res = run_trial(params, trial_id=t_id, loaders=loaders,\n                        num_classes=num_classes, class_weights=class_weights,\n                        device=device, out_dir=OUT_DIR)\n        print(f\"[rank {rank}] -> val={res['best_val_acc']:.4f} @ epoch {res['best_epoch']} | test={res['test_acc']:.4f}\")\n        TRIALS_LOCAL.append(res)\n    elapsed = (time.time() - start) / 60.0\n    print(f\"[rank {rank}] Done in {elapsed:.1f} min\")\n\n    # Gather to rank 0 and finalize CSV / best ckpt\n    obj_list = [None for _ in range(world_size)]\n    dist.all_gather_object(obj_list, TRIALS_LOCAL)\n\n    if is_main:\n        new_rows = [row for sub in obj_list for row in sub]\n        df_new = pd.DataFrame(new_rows)\n\n        if CSV_PATH.exists():\n            df_old = safe_read_results_csv(CSV_PATH)\n            df = pd.concat([df_old, df_new], ignore_index=True)\n            # Drop duplicate param rows (keep last occurrence)\n            df.drop_duplicates(subset=PARAM_KEYS, keep=\"last\", inplace=True)\n        else:\n            df = df_new\n\n        # Ensure numeric types where expected\n        for c in [\"lr\",\"weight_decay\",\"dropout\",\"label_smoothing\",\"batch_size\",\"best_val_acc\",\"test_acc\"]:\n            if c in df.columns:\n                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n        if \"best_epoch\" in df.columns:\n            df[\"best_epoch\"] = pd.to_numeric(df[\"best_epoch\"], errors=\"coerce\", downcast=\"integer\")\n\n        # Save with explicit quoting/escaping\n        df.to_csv(CSV_PATH, index=False, quoting=csv.QUOTE_MINIMAL, escapechar=\"\\\\\", line_terminator=\"\\n\")\n\n        # Pick best by val_acc then test_acc\n        best = df.sort_values([\"best_val_acc\", \"test_acc\"], ascending=False).iloc[0].to_dict()\n        best_ckpt_src = Path(str(best[\"ckpt_path\"]))\n        best_ckpt_dst = OUT_DIR / \"best_resnet50.pt\"\n        if best_ckpt_src.exists():\n            # replace() moves across filesystems too\n            best_ckpt_src.replace(best_ckpt_dst)\n\n        print(\"\\n=== SUMMARY (rank 0) ===\")\n        print(f\"CSV: {CSV_PATH}\")\n        print(f\"Best trial: {json.dumps(best, indent=2)}\")\n        print(f\"Best checkpoint: {best_ckpt_dst}\")\n\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "try:\nmain()\nfinally:\n# Ensure clean shutdown even on exceptions during setup/run\nif dist.is_available() and dist.is_initialized():\ndist.barrier()\ndist.destroy_process_group()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
