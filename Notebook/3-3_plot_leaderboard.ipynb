{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\"\"\"\nUsage:\n  python plot_leaderboard.py --proj_root /content/drive/MyDrive/Alzheimers \\\n    --leaderboard Results/Model_Leaderboard/leaderboard.csv \\\n    --out Results/Model_Leaderboard/Figures \\\n    --top_n 20\n\"\"\"\nimport os\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef short_label(s, max_len=28):\n    s = str(s)\n    return s if len(s) <= max_len else s[:max_len-1] + \"â€¦\"\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True)\n    ap.add_argument(\"--leaderboard\", type=str, default=\"Results/Model_Leaderboard/leaderboard.csv\")\n    ap.add_argument(\"--out\", type=str, default=\"Results/Model_Leaderboard/Figures\")\n    ap.add_argument(\"--top_n\", type=int, default=20)\n    args = ap.parse_args()\n\n    proj = Path(os.path.expanduser(args.proj_root))\n    src_csv = proj / args.leaderboard\n    out_dir = proj / args.out\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    df = pd.read_csv(src_csv)\n\n    # numeric cleanup\n    numeric_cols = [\n        \"img_size\",\"batch_size\",\"global_batch_size\",\n        \"lr\",\"dropout\",\"weight_decay\",\"label_smoothing\",\n        \"best_val_acc\",\"best_epoch\",\"test_acc\",\"precision\",\"recall\",\"f1\",\n        \"epochs\",\"val_split\",\"num_workers\"\n    ]\n    for c in numeric_cols:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    for c in [\"name\",\"source\",\"family\",\"arch\",\"run_dir\",\"cfg_hash\"]:\n        if c not in df.columns:\n            df[c] = \"\"\n\n    df_acc = df.dropna(subset=[\"test_acc\"]).copy()\n\n    # ---------- Figure 1: Top-N bar chart ----------\n    top_df = df_acc.sort_values(\"test_acc\", ascending=False).head(args.top_n).copy()\n    top_df[\"xlab\"] = top_df.apply(lambda r: short_label(f\"{r.get('name', r.get('arch','?'))} [{r.get('source','?')}]\"), axis=1)\n\n    plt.figure(figsize=(12, 6))\n    plt.bar(range(len(top_df)), top_df[\"test_acc\"])\n    plt.xticks(range(len(top_df)), top_df[\"xlab\"], rotation=45, ha=\"right\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(f\"Top {args.top_n} Models by Test Accuracy\")\n    for i, v in enumerate(top_df[\"test_acc\"]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.tight_layout()\n    plt.savefig(out_dir / \"top_models_test_acc.png\", dpi=200)\n    plt.close()\n\n    # ---------- Figure 2: Test vs Best-Val scatter ----------\n    singles = df_acc[df_acc.get(\"source\",\"\") == \"single_arch\"].copy()\n    hybrids = df_acc[df_acc.get(\"source\",\"\") != \"single_arch\"].copy()\n\n    plt.figure(figsize=(7, 6))\n    if not singles.empty:\n        plt.scatter(singles[\"best_val_acc\"], singles[\"test_acc\"], label=\"Single\", marker=\"o\", alpha=0.8)\n    if not hybrids.empty:\n        plt.scatter(hybrids[\"best_val_acc\"], hybrids[\"test_acc\"], label=\"Hybrid\", marker=\"^\", alpha=0.8)\n    mn = np.nanmin([df_acc[\"best_val_acc\"].min(), df_acc[\"test_acc\"].min()])\n    mx = np.nanmax([df_acc[\"best_val_acc\"].max(), df_acc[\"test_acc\"].max()])\n    if not np.isnan(mn) and not np.isnan(mx):\n        plt.plot([mn, mx], [mn, mx], linestyle=\"--\")\n    plt.xlabel(\"Best Val Accuracy\")\n    plt.ylabel(\"Test Accuracy\")\n    plt.title(\"Generalization: Test vs. Best-Validation\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_dir / \"test_vs_val_scatter.png\", dpi=200)\n    plt.close()\n\n    # ---------- Figure 3: Family boxplot ----------\n    fam_groups, fam_labels = [], []\n    if \"family\" in df_acc.columns:\n        fam_stats = df_acc.groupby(\"family\")[\"test_acc\"].median().sort_values(ascending=False)\n        for fam in fam_stats.index.tolist():\n            vals = df_acc[df_acc[\"family\"] == fam][\"test_acc\"].dropna().values\n            if len(vals) > 0:\n                fam_groups.append(vals)\n                fam_labels.append(fam)\n    if fam_groups:\n        plt.figure(figsize=(10, 6))\n        try:\n            # Matplotlib >= 3.9\n            plt.boxplot(fam_groups, tick_labels=fam_labels, showmeans=True)\n        except TypeError:\n            # Matplotlib < 3.9\n            plt.boxplot(fam_groups, labels=fam_labels, showmeans=True)\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(\"Test Accuracy by Architecture Family\")\n        plt.tight_layout()\n        plt.savefig(out_dir / \"family_boxplot.png\", dpi=200)\n        plt.close()\n\n    # ---------- Figure 4: Family means with error bars ----------\n    if \"family\" in df_acc.columns:\n        fam_agg = (df_acc.groupby(\"family\")[\"test_acc\"]\n                   .agg([\"count\",\"mean\",\"std\"])\n                   .sort_values(\"mean\", ascending=False)\n                   .reset_index())\n        if not fam_agg.empty:\n            plt.figure(figsize=(10, 6))\n            x = np.arange(len(fam_agg))\n            y = fam_agg[\"mean\"].values\n            yerr = fam_agg[\"std\"].values\n            plt.bar(x, y, yerr=yerr, capsize=4)\n            plt.xticks(x, fam_agg[\"family\"].tolist(), rotation=45, ha=\"right\")\n            plt.ylabel(\"Mean Test Accuracy\")\n            plt.title(\"Family Averages (error bars = stdev)\")\n            plt.tight_layout()\n            plt.savefig(out_dir / \"family_means_errbars.png\", dpi=200)\n            plt.close()\n\n    # ---------- Figure 5: Test Acc vs Image Size ----------\n    if \"img_size\" in df_acc.columns:\n        plt.figure(figsize=(8, 6))\n        if not singles.empty and \"img_size\" in singles.columns:\n            plt.scatter(singles[\"img_size\"], singles[\"test_acc\"], label=\"Single\", marker=\"o\", alpha=0.8)\n        if not hybrids.empty and \"img_size\" in hybrids.columns:\n            plt.scatter(hybrids[\"img_size\"], hybrids[\"test_acc\"], label=\"Hybrid\", marker=\"^\", alpha=0.8)\n        plt.xlabel(\"Image Size\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(\"Test Accuracy vs. Input Image Size\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(out_dir / \"test_acc_vs_img_size.png\", dpi=200)\n        plt.close()\n\n    print(\"Saved figures in:\", out_dir)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
