{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n\"\"\"\nSummarize ArchSweep runs (and optional legacy hybrid runs) into a leaderboard + JSONs\nfor quick retraining and hybrid selection.\n\nOutputs (configurable):\n- <proj_root>/<leaderboard_dir>/leaderboard.csv         # ranked table\n- <proj_root>/<leaderboard_dir>/leaderboard.md          # markdown preview\n- <proj_root>/<leaderboard_dir>/top_per_arch.json       # best single config per architecture\n- <proj_root>/<leaderboard_dir>/top_for_hybrids.json    # shortlists per family (for hybrid pairing)\n- <proj_root>/<leaderboard_dir>/best_retrained.json     # arch -> {lr, dropout, label_smoothing, weight_decay, batch_size}\n- (optional) <proj_root>/<leaderboard_dir>/leaderboard_wide.{csv,md}  # main + extra-test metrics per row\n\nNotes:\n- Use --leaderboard_dir to avoid overwriting your legacy leaderboard (default: Results/Model_Leaderboard).\n- Concatenation skips empty / all-NA DataFrames to avoid pandas FutureWarnings.\n- Pass --combine_extras_wide to emit a 'wide' table that merges extra-test scores into the same row.\n\nUsage examples\n--------------\n# 1) Classic ArchSweep summary (legacy default output location)\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/ArchSweep \\\n  --families \"ResNet,DenseNet,Inception,ResNeXt,EffNet,MobileNetV2,MobileNetV3,VGG,CNN_Small,Other\" \\\n  --per_family 3 \\\n  --top_overall 100\n\n# 2) Singles-only summary to a SEPARATE leaderboard folder (skip shortlists)\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/Singles_Luke \\\n  --per_family 0 \\\n  --top_overall 100 \\\n  --combine_extras_wide \\\n  --leaderboard_dir Results/Model_Leaderboard_Singles_Luke\n\n# 3) More Singles sets\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/Singles_Marco \\\n  --per_family 0 \\\n  --top_overall 100 \\\n  --combine_extras_wide \\\n  --leaderboard_dir Results/Model_Leaderboard_Singles_Marco\n\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/Singles_Falah \\\n  --per_family 0 \\\n  --top_overall 100 \\\n  --combine_extras_wide \\\n  --leaderboard_dir Results/Model_Leaderboard_Singles_Falah\n\n# 4) Mix Singles + legacy hybrid dirs (with optional HP JSONs) into a custom leaderboard dir\npython 3-2_summarize_models.py \\\n  --proj_root /content/drive/MyDrive/Alzheimers \\\n  --sweep_dir Results/ArchSweep \\\n  --hybrid_dirs \"Results/Hybrid_Results_0826|hybrid_luke,Results/Hybrid_Results_0902|hybrid_luke+marco\" \\\n  --hybrid_hp \"hybrid_luke=/mnt/data/Hyperparameters_hybrid_for_Luke.json,hybrid_luke+marco=/mnt/data/Hyperparameters_hybrid_for_LukeMarco.json\" \\\n  --families \"ResNet,DenseNet,Inception,ResNeXt,EffNet,MobileNetV2,MobileNetV3,VGG,CNN_Small,Other\" \\\n  --per_family 3 \\\n  --top_overall 100 \\\n  --leaderboard_dir Results/Model_Leaderboard_Custom\n\"\"\"\n\nimport os, re, json, argparse\nfrom pathlib import Path\nimport pandas as pd\n\n# -------------------------\n# Family mapping\n# -------------------------\ndef family_of(arch: str) -> str:\n    a = (arch or \"\").lower()\n    if \"resnext\" in a: return \"ResNeXt\"\n    if \"resnet\"  in a: return \"ResNet\"\n    if \"densenet\" in a: return \"DenseNet\"\n    if \"inception\" in a: return \"Inception\"\n    if \"efficientnet\" in a or \"effnet\" in a: return \"EffNet\"\n    if \"mobilenetv3\" in a: return \"MobileNetV3\"\n    if \"mobilenetv2\" in a: return \"MobileNetV2\"\n    if \"vgg\" in a: return \"VGG\"\n    if \"cnn_small\" in a: return \"CNN_Small\"\n    return \"Other\"\n\n# -------------------------\n# ArchSweep parser\n# -------------------------\ndef parse_arch_sweep(sweep_dir: Path) -> pd.DataFrame:\n    csv_path = sweep_dir / \"arch_sweep_results.csv\"\n    if not csv_path.exists():\n        return pd.DataFrame()\n    df = pd.read_csv(csv_path)\n\n    # ensure columns exist\n    for col in [\"arch\",\"cfg_hash\",\"lr\",\"dropout\",\"weight_decay\",\"label_smoothing\",\n                \"batch_size\",\"best_val_acc\",\"best_epoch\",\"test_acc\",\"img_size\",\"run_dir\"]:\n        if col not in df.columns:\n            df[col] = None\n\n    # enrich with params.json (epochs, val_split, global_batch_size if present)\n    rows = []\n    for _, r in df.iterrows():\n        run_dir = Path(str(r[\"run_dir\"])) if pd.notna(r.get(\"run_dir\")) else None\n        params = {}\n        if run_dir and (run_dir / \"params.json\").exists():\n            try:\n                params = json.loads((run_dir / \"params.json\").read_text())\n            except Exception:\n                params = {}\n        row = dict(r)\n        row[\"epochs\"] = params.get(\"epochs\")\n        row[\"val_split\"] = params.get(\"val_split\")\n        row[\"global_batch_size\"] = params.get(\"global_batch_size\", row.get(\"batch_size\"))\n        row[\"family\"] = family_of(str(row[\"arch\"]))\n        row[\"source\"] = \"single_arch\"\n        rows.append(row)\n\n    out = pd.DataFrame(rows)\n    sort_cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in out.columns]\n    if sort_cols:\n        out = out.sort_values(sort_cols, ascending=False).reset_index(drop=True)\n    return out\n\n# -------------------------\n# Hybrid hyperparameter injection\n# -------------------------\ndef _std_hparams_from_json(p: Path) -> dict:\n    \"\"\"Normalize keys from legacy hybrid JSONs to our leaderboard columns.\"\"\"\n    try:\n        blob = json.loads(p.read_text())\n    except Exception:\n        return {}\n\n    def pick_lr(d):\n        for k in [\"learning_rate\", \"initial_learning_rate\", \"lr\"]:\n            if k in d and d[k] is not None:\n                try:\n                    return float(d[k])\n                except Exception:\n                    pass\n        return None\n\n    def pick_img_size(d):\n        v = d.get(\"image_size\")\n        if v is None:\n            return None\n        if isinstance(v, (list, tuple)) and len(v) > 0:\n            try:\n                return int(v[0])\n            except Exception:\n                return None\n        try:\n            return int(v)\n        except Exception:\n            return None\n\n    out = {\n        \"lr\": pick_lr(blob),\n        \"dropout\": float(blob[\"dropout_rate\"]) if \"dropout_rate\" in blob and blob[\"dropout_rate\"] is not None else None,\n        \"batch_size\": int(blob[\"batch_size\"]) if \"batch_size\" in blob and blob[\"batch_size\"] is not None else None,\n        \"epochs\": int(blob[\"epochs\"]) if \"epochs\" in blob and blob[\"epochs\"] is not None else None,\n        \"img_size\": pick_img_size(blob),\n        \"val_split\": float(blob[\"validation_split\"]) if \"validation_split\" in blob and blob[\"validation_split\"] is not None else None,\n        \"num_workers\": int(blob[\"num_workers\"]) if \"num_workers\" in blob and blob[\"num_workers\"] is not None else None,\n        \"weight_decay\": float(blob[\"weight_decay\"]) if \"weight_decay\" in blob and blob[\"weight_decay\"] is not None else None,\n        \"label_smoothing\": float(blob[\"label_smoothing\"]) if \"label_smoothing\" in blob and blob[\"label_smoothing\"] is not None else None,\n    }\n    return out\n\ndef parse_hybrid_hp_map(spec: str, proj_root: Path) -> dict:\n    \"\"\"spec: 'label=/abs/or/rel.json,label2=/path2.json'\"\"\"\n    if not spec.strip():\n        return {}\n    out = {}\n    items = [s.strip() for s in spec.split(\",\") if s.strip()]\n    for it in items:\n        if \"=\" not in it:\n            continue\n        label, path_str = it.split(\"=\", 1)\n        label = label.strip()\n        p = Path(os.path.expanduser(path_str.strip()))\n        if not p.is_absolute():\n            p = (proj_root / p).resolve()\n        if p.exists():\n            out[label] = _std_hparams_from_json(p)\n        else:\n            out[label] = {}\n    return out\n\n# -------------------------\n# Hybrid results parser (summary.txt)\n# -------------------------\ndef parse_one_hybrid_dir(hybrid_dir: Path, label: str, hp_overrides_by_label: dict) -> pd.DataFrame:\n    \"\"\"Expect each subdir of hybrid_dir to have summary.txt with scalar metrics.\"\"\"\n    rows = []\n    if not hybrid_dir.exists():\n        return pd.DataFrame()\n\n    hp = hp_overrides_by_label.get(label, {}) if label else {}\n\n    for model_dir in sorted(hybrid_dir.iterdir()):\n        if not model_dir.is_dir():\n            continue\n        p = model_dir / \"summary.txt\"\n        if not p.exists():\n            continue\n        text = p.read_text()\n        def grab(name):\n            m = re.search(rf\"{name}\\s*:\\s*([0-9.]+)\", text)\n            return float(m.group(1)) if m else None\n        row = {\n            \"arch\": model_dir.name,\n            \"source\": label or \"hybrid_legacy\",\n            \"test_acc\": grab(\"Test Acc\"),\n            \"precision\": grab(\"Precision\"),\n            \"recall\": grab(\"Recall\"),\n            \"f1\": grab(\"F1-score\"),\n            \"run_dir\": str(model_dir),\n            \"family\": \"Hybrid\",\n        }\n        for k, v in hp.items():\n            if v is not None:\n                if k == \"img_size\":\n                    row[\"img_size\"] = v\n                else:\n                    row[k] = v\n        rows.append(row)\n    return pd.DataFrame(rows)\n\ndef parse_hybrid_dirs(hybrid_specs: str, proj_root: Path, hp_overrides_by_label: dict) -> pd.DataFrame:\n    \"\"\"\n    Accepts a comma-separated list like:\n      \"Results/Hybrid_Results_0826|hybrid_luke,Results/Hybrid_Results_0902|hybrid_luke+marco\"\n    \"\"\"\n    if not hybrid_specs.strip():\n        return pd.DataFrame()\n    parts = [p.strip() for p in hybrid_specs.split(\",\") if p.strip()]\n    frames = []\n    for item in parts:\n        if \"|\" in item:\n            path_str, label = item.split(\"|\", 1)\n        else:\n            path_str, label = item, \"hybrid_legacy\"\n        p = Path(path_str)\n        if not p.is_absolute():\n            p = (proj_root / p).resolve()\n        frames.append(parse_one_hybrid_dir(p, label, hp_overrides_by_label))\n    if frames:\n        cleaned = [f for f in frames if not f.empty and not f.isna().all().all()]\n        return pd.concat(cleaned, ignore_index=True, sort=False) if cleaned else pd.DataFrame()\n    return pd.DataFrame()\n\n# -------------------------\n# Extras (wide view) helpers\n# -------------------------\ndef _parse_floats_from_text(text: str) -> dict:\n    def grab(name):\n        m = re.search(rf\"{name}\\s*:\\s*([0-9.]+)\", text)\n        return float(m.group(1)) if m else None\n    return {\n        \"test_acc\": grab(\"Test Acc\"),\n        \"precision\": grab(\"Precision\"),\n        \"recall\": grab(\"Recall\"),\n        \"f1\": grab(\"F1-score\"),\n    }\n\ndef _read_extra_metrics_file(p: Path) -> dict:\n    try:\n        if p.suffix.lower() == \".json\":\n            blob = json.loads(p.read_text())\n            return {\n                \"test_acc\": float(blob.get(\"test_acc\")) if blob.get(\"test_acc\") is not None else None,\n                \"precision\": float(blob.get(\"precision\")) if blob.get(\"precision\") is not None else None,\n                \"recall\": float(blob.get(\"recall\")) if blob.get(\"recall\") is not None else None,\n                \"f1\": float(blob.get(\"f1\")) if blob.get(\"f1\") is not None else None,\n            }\n        else:\n            return _parse_floats_from_text(p.read_text())\n    except Exception:\n        return {}\n\ndef _label_from_path(p: Path) -> str:\n    \"\"\"\n    Infer label from path (extras/Luke/summary.txt -> 'Luke',\n    summary_Marco.txt -> 'Marco', extra_Falah_metrics.json -> 'Falah').\n    \"\"\"\n    name = p.stem\n    if p.parent.name and p.parent.parent and p.parent.parent.name == \"extras\":\n        return p.parent.name\n    m = re.search(r\"summary[_\\-]([A-Za-z0-9\\-\\+]+)$\", name)\n    if m:\n        return m.group(1)\n    m = re.search(r\"extra[_\\-]([A-Za-z0-9\\-\\+]+)\", name)\n    if m:\n        return m.group(1)\n    return \"extra\"\n\ndef collect_extra_tests(run_dir: str, patterns_csv: str) -> dict:\n    \"\"\"\n    Returns {label: {'test_acc':..., 'precision':..., 'recall':..., 'f1':...}}\n    Scans given glob patterns under run_dir.\n    \"\"\"\n    out = {}\n    if not run_dir:\n        return out\n    base = Path(run_dir)\n    patterns = [s.strip() for s in patterns_csv.split(\",\") if s.strip()]\n    for pat in patterns:\n        for p in base.glob(pat):\n            if not p.is_file():\n                continue\n            metrics = _read_extra_metrics_file(p)\n            if not any(v is not None for v in metrics.values()):\n                continue\n            lab = _label_from_path(p)\n            out[lab] = metrics\n    return out\n\n# -------------------------\n# Helpers\n# -------------------------\ndef pick_best_per_arch(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty:\n        return df\n    cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in df.columns]\n    if cols:\n        sdf = df.sort_values(cols, ascending=False).copy()\n    else:\n        sdf = df.copy()\n    return sdf.drop_duplicates(subset=[\"arch\"], keep=\"first\").reset_index(drop=True)\n\ndef dataframe_to_markdown(df: pd.DataFrame, max_rows=30) -> str:\n    try:\n        return df.head(max_rows).to_markdown(index=False)\n    except Exception:\n        return df.head(max_rows).to_string(index=False)\n\n# -------------------------\n# Main\n# -------------------------\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True)\n    ap.add_argument(\"--sweep_dir\", type=str, default=\"Results/ArchSweep\")\n    ap.add_argument(\"--hybrid_dirs\", type=str, default=\"\",\n                    help=\"Comma list of PATH or PATH|LABEL, e.g., \"\n                         \"'Results/Hybrid_Results_0826|hybrid_luke,Results/Hybrid_Results_0902|hybrid_luke+marco'\")\n    ap.add_argument(\"--hybrid_hp\", type=str, default=\"\",\n                    help='Comma list of LABEL=JSONPATH providing hyperparameters for hybrid rows, '\n                         'e.g., \"hybrid_luke=/path/Luke.json,hybrid_luke+marco=/path/LukeMarco.json\"')\n    ap.add_argument(\"--families\", type=str, default=\"ResNet,DenseNet,Inception\",\n                    help=\"Comma-separated families to shortlist for hybrids (e.g., add ResNeXt,EffNet,MobileNetV2,MobileNetV3,VGG)\")\n    ap.add_argument(\"--per_family\", type=int, default=2, help=\"Top-N per family for hybrid shortlists (0 to skip shortlists)\")\n    ap.add_argument(\"--top_overall\", type=int, default=50, help=\"Rows to keep in leaderboard display (0 = keep all)\")\n    ap.add_argument(\"--leaderboard_dir\", type=str, default=\"Results/Model_Leaderboard\",\n                    help=\"Output directory for leaderboard files (CSV/MD and JSONs). Default: Results/Model_Leaderboard\")\n    ap.add_argument(\"--combine_extras_wide\", action=\"store_true\",\n                    help=\"Create leaderboard_wide.{csv,md} that merges main and extra-test metrics for each model into one row.\")\n    ap.add_argument(\"--extras_patterns\", type=str,\n                    default=\"extras/*/summary.txt,summary_*.txt,extra_*_metrics.json\",\n                    help=\"Comma-separated glob patterns (relative to run_dir) to find extra-test metrics.\")\n    args = ap.parse_args()\n\n    proj_root = Path(os.path.expanduser(args.proj_root))\n    sweep_dir = proj_root / args.sweep_dir\n    out_dir = (proj_root / args.leaderboard_dir).resolve()\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # parse hyperparam overrides for hybrid labels\n    hp_overrides_by_label = parse_hybrid_hp_map(args.hybrid_hp, proj_root)\n\n    singles = parse_arch_sweep(sweep_dir)\n    hybrids = parse_hybrid_dirs(args.hybrid_dirs, proj_root, hp_overrides_by_label)\n\n    # robust concat to avoid FutureWarning / dtype issues\n    frames = [df for df in [singles, hybrids] if not df.empty and not df.isna().all().all()]\n    all_df = pd.concat(frames, ignore_index=True, sort=False) if frames else pd.DataFrame()\n\n    # Name for display\n    if \"cfg_hash\" in all_df.columns:\n        def mkname(r):\n            if r.get(\"source\") == \"single_arch\":\n                return f\"{r.get('arch','?')} ({str(r.get('cfg_hash',''))[:6]})\"\n            return f\"{r.get('arch','?')} [{r.get('source','hybrid')}]\"\n        all_df[\"name\"] = all_df.apply(mkname, axis=1)\n    else:\n        all_df[\"name\"] = all_df.get(\"arch\", \"?\")\n\n    # Sort by test_acc desc then best_val_acc\n    sort_cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in all_df.columns]\n    if sort_cols and not all_df.empty:\n        all_df = all_df.sort_values(sort_cols, ascending=False).reset_index(drop=True)\n\n    # Leaderboard view\n    leaderboard = all_df.copy()\n    if args.top_overall and args.top_overall > 0 and not leaderboard.empty:\n        leaderboard = leaderboard.head(args.top_overall).copy()\n\n    # keep standard columns, + any per-dataset accuracy columns from 3-1 (acc_*)\n    keep_cols = [c for c in [\n        \"name\",\"source\",\"family\",\"arch\",\"img_size\",\"batch_size\",\"global_batch_size\",\n        \"lr\",\"dropout\",\"weight_decay\",\"label_smoothing\",\n        \"best_val_acc\",\"best_epoch\",\"test_acc\",\"precision\",\"recall\",\"f1\",\"run_dir\",\"cfg_hash\",\"epochs\",\"val_split\",\n        \"num_workers\"\n    ] if c in leaderboard.columns]\n    extra_acc_cols = [c for c in leaderboard.columns if c.startswith(\"acc_\")]\n    keep_cols = keep_cols + extra_acc_cols\n\n    if not leaderboard.empty:\n        leaderboard = leaderboard[keep_cols]\n\n    # Save leaderboard\n    leaderboard_csv = out_dir / \"leaderboard.csv\"\n    leaderboard_md  = out_dir / \"leaderboard.md\"\n    leaderboard.to_csv(leaderboard_csv, index=False)\n    leaderboard_md.write_text(dataframe_to_markdown(leaderboard))\n\n    # Optional: \"wide\" leaderboard that merges extra-test metrics per model\n    if args.combine_extras_wide and not leaderboard.empty:\n        rows_wide = []\n        for _, r in leaderboard.iterrows():\n            base_row = {\n                \"name\": r.get(\"name\"),\n                \"arch\": r.get(\"arch\"),\n                \"family\": r.get(\"family\"),\n                \"source\": r.get(\"source\"),\n                \"cfg_hash\": r.get(\"cfg_hash\"),\n                \"run_dir\": r.get(\"run_dir\"),\n                \"img_size\": r.get(\"img_size\"),\n                \"batch_size\": r.get(\"batch_size\"),\n                \"global_batch_size\": r.get(\"global_batch_size\"),\n                \"lr\": r.get(\"lr\"),\n                \"dropout\": r.get(\"dropout\"),\n                \"weight_decay\": r.get(\"weight_decay\"),\n                \"label_smoothing\": r.get(\"label_smoothing\"),\n                \"test_acc__main\": r.get(\"test_acc\"),\n                \"precision__main\": r.get(\"precision\"),\n                \"recall__main\": r.get(\"recall\"),\n                \"f1__main\": r.get(\"f1\"),\n            }\n            # Try to read extras from files under run_dir\n            extras = collect_extra_tests(r.get(\"run_dir\"), args.extras_patterns)\n\n            # Fallback: if no files found, use any acc_* columns on this row\n            if not extras:\n                for col in [c for c in leaderboard.columns if c.startswith(\"acc_\")]:\n                    lab = re.sub(r\"^acc_\", \"\", col)   # acc_MarcoTest -> MarcoTest\n                    lab = re.sub(r\"Test$\", \"\", lab)   # MarcoTest -> Marco\n                    base_row[f\"test_acc__{lab}\"] = r.get(col)\n            else:\n                for lab, m in sorted(extras.items()):\n                    base_row[f\"test_acc__{lab}\"]  = m.get(\"test_acc\")\n                    base_row[f\"precision__{lab}\"] = m.get(\"precision\")\n                    base_row[f\"recall__{lab}\"]    = m.get(\"recall\")\n                    base_row[f\"f1__{lab}\"]        = m.get(\"f1\")\n\n            rows_wide.append(base_row)\n\n        leaderboard_wide = pd.DataFrame(rows_wide)\n        sort_cols_wide = [\"test_acc__main\"] + sorted(\n            [c for c in leaderboard_wide.columns if c.startswith(\"test_acc__\") and c != \"test_acc__main\"],\n            reverse=True\n        )\n        leaderboard_wide = leaderboard_wide.sort_values(sort_cols_wide, ascending=False, na_position=\"last\").reset_index(drop=True)\n        (out_dir / \"leaderboard_wide.csv\").write_text(leaderboard_wide.to_csv(index=False))\n        (out_dir / \"leaderboard_wide.md\").write_text(dataframe_to_markdown(leaderboard_wide, max_rows=50))\n\n    # Best per arch (singles only)\n    singles_only = all_df[all_df[\"source\"]==\"single_arch\"].copy() if \"source\" in all_df.columns else all_df.copy()\n    best_per_arch = pick_best_per_arch(singles_only)\n\n    top_per_arch_json = {\n        row[\"arch\"]: {\n            \"cfg_hash\": row.get(\"cfg_hash\"),\n            \"run_dir\": row.get(\"run_dir\"),\n            \"lr\": float(row.get(\"lr\")) if pd.notna(row.get(\"lr\")) else None,\n            \"dropout\": float(row.get(\"dropout\")) if pd.notna(row.get(\"dropout\")) else None,\n            \"label_smoothing\": float(row.get(\"label_smoothing\")) if pd.notna(row.get(\"label_smoothing\")) else None,\n            \"weight_decay\": float(row.get(\"weight_decay\")) if pd.notna(row.get(\"weight_decay\")) else None,\n            \"batch_size\": int(row.get(\"global_batch_size\") if pd.notna(row.get(\"global_batch_size\")) else row.get(\"batch_size\")) if pd.notna(row.get(\"batch_size\")) or pd.notna(row.get(\"global_batch_size\")) else None,\n            \"best_val_acc\": float(row.get(\"best_val_acc\")) if pd.notna(row.get(\"best_val_acc\")) else None,\n            \"test_acc\": float(row.get(\"test_acc\")) if pd.notna(row.get(\"test_acc\")) else None,\n        }\n        for _, row in best_per_arch.iterrows()\n    }\n    (out_dir / \"top_per_arch.json\").write_text(json.dumps(top_per_arch_json, indent=2))\n\n    # Shortlists per family to build hybrids (skip if per_family <= 0)\n    wanted_families = [f.strip() for f in args.families.split(\",\") if f.strip()]\n    shortlists = {}\n    if args.per_family and args.per_family > 0 and wanted_families:\n        for fam in wanted_families:\n            fam_rows = singles_only[singles_only[\"family\"] == fam].copy()\n            if fam_rows.empty:\n                continue\n            fam_rows = fam_rows.sort_values(sort_cols, ascending=False).head(args.per_family)\n            shortlists[fam] = [\n                {\n                    \"arch\": r[\"arch\"],\n                    \"cfg_hash\": r.get(\"cfg_hash\"),\n                    \"run_dir\": r.get(\"run_dir\"),\n                    \"lr\": float(r.get(\"lr\")) if pd.notna(r.get(\"lr\")) else None,\n                    \"dropout\": float(r.get(\"dropout\")) if pd.notna(r.get(\"dropout\")) else None,\n                    \"label_smoothing\": float(r.get(\"label_smoothing\")) if pd.notna(r.get(\"label_smoothing\")) else None,\n                    \"weight_decay\": float(r.get(\"weight_decay\")) if pd.notna(r.get(\"weight_decay\")) else None,\n                    \"batch_size\": int(r.get(\"global_batch_size\") if pd.notna(r.get(\"global_batch_size\")) else r.get(\"batch_size\")) if pd.notna(r.get(\"batch_size\")) or pd.notna(r.get(\"global_batch_size\")) else None,\n                    \"best_val_acc\": float(r.get(\"best_val_acc\")) if pd.notna(r.get(\"best_val_acc\")) else None,\n                    \"test_acc\": float(r.get(\"test_acc\")) if pd.notna(r.get(\"test_acc\")) else None,\n                }\n                for _, r in fam_rows.iterrows()\n            ]\n    (out_dir / \"top_for_hybrids.json\").write_text(json.dumps(shortlists, indent=2))\n\n    # Emit a best_retrained.json (schema your trainer expects)\n    best_retrained = {}\n    for arch, blob in top_per_arch_json.items():\n        best_retrained[arch] = {\n            \"lr\": blob.get(\"lr\"),\n            \"label_smoothing\": blob.get(\"label_smoothing\"),\n            \"dropout\": blob.get(\"dropout\"),\n            \"weight_decay\": blob.get(\"weight_decay\"),\n            \"batch_size\": blob.get(\"batch_size\"),\n        }\n    (out_dir / \"best_retrained.json\").write_text(json.dumps(best_retrained, indent=2))\n\n    print(\"Saved:\")\n    print(f\"- {leaderboard_csv}\")\n    print(f\"- {leaderboard_md}\")\n    if args.combine_extras_wide:\n        print(f\"- {out_dir / 'leaderboard_wide.csv'}\")\n        print(f\"- {out_dir / 'leaderboard_wide.md'}\")\n    print(f\"- {out_dir / 'top_per_arch.json'}\")\n    print(f\"- {out_dir / 'top_for_hybrids.json'}\")\n    print(f\"- {out_dir / 'best_retrained.json'}\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
