{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\n\"\"\"\nfinal_eval.py â€” One-stop final evaluation + search summary for ResNet50\n\n- Loads final_best_resnet50.pt (payload from retrain) OR a raw HPO best .pt state_dict\n- Evaluates on a test ImageFolder\n- Auto-detects head style (linear vs dropout->linear) from checkpoint keys\n- Auto-infers dropout and eval batch size from checkpoint payload if present\n- Writes to <checkpoint_dir>/reports/:\n  * classification_report.txt\n  * confusion_matrix.png\n  * roc_curves_{perclass,micro,macro}.png\n  * pr_curves_{perclass,micro,macro}.png\n  * calibration_reliability.png\n  * ece.txt\n  * metrics.json (accuracy, ROC/PR AUCs, ECE)\n  * (optional) hpo_top10.csv + hpo_summary.md from --hpo_csv\n  * (optional) retrain_summary_sorted.csv from --retrain_csv\n  * REPORT.md (high-level summary with links)\n\"\"\"\n\nimport os\nimport csv\nimport json\nimport argparse\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n    roc_curve, auc,\n    precision_recall_curve, average_precision_score,\n    accuracy_score\n)\n\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import functional as F, InterpolationMode\n\n# --- Quiet specific, known deprecations cleanly (optional but requested) ---\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"`estimator_name` is deprecated\",\n    category=FutureWarning,\n    module=\"sklearn\",\n)\n\n# ----------------------------\n# Transforms (Pad -> Resize)\n# ----------------------------\nclass PadToSquare:\n    def __call__(self, img):\n        w, h = F.get_image_size(img)\n        s = max(w, h)\n        pad_l = (s - w) // 2\n        pad_r = s - w - pad_l\n        pad_t = (s - h) // 2\n        pad_b = s - h - pad_t\n        return F.pad(img, [pad_l, pad_t, pad_r, pad_b], fill=0)\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD  = [0.229, 0.224, 0.225]\n\ndef make_eval_tf(size: int = 224):\n    return transforms.Compose([\n        PadToSquare(),\n        transforms.Resize((size, size), interpolation=InterpolationMode.BICUBIC, antialias=True),\n        transforms.ToTensor(),\n        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n    ])\n\n# ----------------------------\n# Model builder (ResNet50) with head style handling\n# ----------------------------\ndef build_resnet50_for_style(num_classes: int, head_style: str, dropout_p: float = 0.0):\n    \"\"\"\n    head_style: \"linear\" (fc: Linear) or \"seq\" (fc: Dropout -> Linear)\n    dropout_p is used only when head_style == \"seq\" (eval mode disables dropout anyway).\n    \"\"\"\n    m = models.resnet50(weights=None)  # load weights from checkpoint\n    in_dim = m.fc.in_features\n    if head_style == \"seq\":\n        dp = float(dropout_p) if dropout_p is not None else 0.3\n        m.fc = nn.Sequential(nn.Dropout(p=dp), nn.Linear(in_dim, num_classes))\n    else:\n        m.fc = nn.Linear(in_dim, num_classes)\n    return m\n\ndef detect_head_style_from_state_dict(sd: dict) -> str:\n    \"\"\"\n    Returns \"seq\" if the checkpoint expects fc.1.* keys (Dropout->Linear),\n    otherwise \"linear\" if it expects fc.weight/fc.bias.\n    \"\"\"\n    has_seq = any(k.startswith(\"fc.1.\") for k in sd.keys())\n    has_lin = \"fc.weight\" in sd and \"fc.bias\" in sd\n    if has_seq and not has_lin:\n        return \"seq\"\n    if has_lin and not has_seq:\n        return \"linear\"\n    # Ambiguous: default to linear\n    return \"linear\"\n\n# ----------------------------\n# Prediction\n# ----------------------------\n@torch.no_grad()\ndef predict(model, loader, device):\n    model.eval()\n    probs_list, preds_list, labels_list = [], [], []\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        logits = model(xb)\n        if isinstance(logits, tuple):  # future-proof (e.g., Inception)\n            logits = logits[0]\n        probs = torch.softmax(logits, dim=1).cpu().numpy()\n        preds = probs.argmax(axis=1)\n        probs_list.append(probs)\n        preds_list.append(preds)\n        labels_list.append(yb.numpy())\n    probs = np.concatenate(probs_list, axis=0)\n    preds = np.concatenate(preds_list, axis=0)\n    labels = np.concatenate(labels_list, axis=0)\n    return probs, preds, labels\n\n# ----------------------------\n# Plot helpers (sklearn name= shim)\n# ----------------------------\ndef _roc_display(ax, fpr, tpr, auc_val, label):\n    # name= (new) with fallback to estimator_name= (old)\n    try:\n        from sklearn.metrics import RocCurveDisplay\n        RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_val, name=label).plot(ax=ax)\n    except TypeError:\n        from sklearn.metrics import RocCurveDisplay\n        RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_val, estimator_name=label).plot(ax=ax)\n\ndef _pr_display(ax, precision, recall, ap, label):\n    # name= (new) with fallback to estimator_name= (old)\n    try:\n        from sklearn.metrics import PrecisionRecallDisplay\n        PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=ap, name=label).plot(ax=ax)\n    except TypeError:\n        from sklearn.metrics import PrecisionRecallDisplay\n        PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=ap, estimator_name=label).plot(ax=ax)\n\n# ----------------------------\n# Plots & metrics\n# ----------------------------\ndef plot_confusion_matrix(y_true, y_pred, class_names, out_path):\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n    disp.plot(cmap=\"Blues\", values_format=\"d\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=160)\n    plt.close()\n\ndef plot_roc_curves(y_true, probs, class_names, out_dir):\n    n_classes = len(class_names)\n    y_bin = np.eye(n_classes)[y_true]\n\n    # Per-class\n    plt.figure(figsize=(8, 6))\n    aucs = []\n    ax = plt.gca()\n    for i in range(n_classes):\n        fpr, tpr, _ = roc_curve(y_bin[:, i], probs[:, i])\n        roc_auc = auc(fpr, tpr); aucs.append(roc_auc)\n        _roc_display(ax, fpr, tpr, roc_auc, class_names[i])\n    plt.title(\"ROC Curves (per-class)\")\n    plt.tight_layout(); plt.savefig(out_dir / \"roc_curves_perclass.png\", dpi=160); plt.close()\n\n    # Micro-average\n    fpr_micro, tpr_micro, _ = roc_curve(y_bin.ravel(), probs.ravel())\n    auc_micro = auc(fpr_micro, tpr_micro)\n    plt.figure()\n    _roc_display(plt.gca(), fpr_micro, tpr_micro, auc_micro, \"micro-average\")\n    plt.title(f\"ROC (micro-average, AUC={auc_micro:.4f})\")\n    plt.tight_layout(); plt.savefig(out_dir / \"roc_curves_micro.png\", dpi=160); plt.close()\n\n    # Macro-average\n    macro_auc = float(np.mean(aucs))\n    plt.figure(figsize=(6, 5))\n    for i in range(n_classes):\n        fpr, tpr, _ = roc_curve(y_bin[:, i], probs[:, i])\n        plt.plot(fpr, tpr, alpha=0.2)\n    plt.plot([0,1],[0,1],\"k--\",lw=1)\n    plt.title(f\"ROC (macro-average, AUC={macro_auc:.4f})\")\n    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n    plt.tight_layout(); plt.savefig(out_dir / \"roc_curves_macro.png\", dpi=160); plt.close()\n\n    return {\"auc_micro\": float(auc_micro), \"auc_macro\": float(macro_auc)}\n\ndef plot_pr_curves(y_true, probs, class_names, out_dir):\n    n_classes = len(class_names)\n    y_bin = np.eye(n_classes)[y_true]\n\n    # Per-class\n    plt.figure(figsize=(8, 6))\n    aps = []\n    ax = plt.gca()\n    for i in range(n_classes):\n        precision, recall, _ = precision_recall_curve(y_bin[:, i], probs[:, i])\n        ap = average_precision_score(y_bin[:, i], probs[:, i]); aps.append(ap)\n        _pr_display(ax, precision, recall, ap, class_names[i])\n    plt.title(\"PR Curves (per-class)\")\n    plt.tight_layout(); plt.savefig(out_dir / \"pr_curves_perclass.png\", dpi=160); plt.close()\n\n    # Micro-average\n    precision_micro, recall_micro, _ = precision_recall_curve(y_bin.ravel(), probs.ravel())\n    ap_micro = average_precision_score(y_bin.ravel(), probs.ravel())\n    plt.figure()\n    _pr_display(plt.gca(), precision_micro, recall_micro, ap_micro, \"micro-average\")\n    plt.title(f\"PR (micro-average, AP={ap_micro:.4f})\")\n    plt.tight_layout(); plt.savefig(out_dir / \"pr_curves_micro.png\", dpi=160); plt.close()\n\n    # Macro (mean of per-class APs)\n    ap_macro = float(np.mean(aps))\n    return {\"ap_micro\": float(ap_micro), \"ap_macro\": float(ap_macro)}\n\ndef expected_calibration_error(probs, labels, n_bins: int = 15):\n    confidences = probs.max(axis=1)\n    predictions = probs.argmax(axis=1)\n    accuracies = (predictions == labels).astype(float)\n\n    bins = np.linspace(0, 1, n_bins + 1)\n    ece = 0.0\n    bin_stats = []\n    for b in range(n_bins):\n        lo, hi = bins[b], bins[b+1]\n        mask = (confidences > lo) & (confidences <= hi) if b>0 else (confidences >= lo) & (confidences <= hi)\n        if not np.any(mask):\n            bin_stats.append((lo, hi, 0, 0.0, 0.0)); continue\n        conf_m = float(confidences[mask].mean())\n        acc_m  = float(accuracies[mask].mean())\n        gap = abs(acc_m - conf_m)\n        ece += (mask.mean()) * gap\n        bin_stats.append((lo, hi, int(mask.sum()), conf_m, acc_m))\n    return float(ece), bin_stats\n\ndef plot_reliability(bin_stats, out_path):\n    confs = [x[3] for x in bin_stats]\n    accs  = [x[4] for x in bin_stats]\n    centers = np.linspace(0, 1, len(confs), endpoint=False) + 0.5/len(confs)\n    plt.figure(figsize=(6,6))\n    plt.plot([0,1],[0,1],\"k--\",lw=1,label=\"perfect\")\n    plt.bar(centers, accs, width=1/len(confs), alpha=0.7, edgecolor=\"k\", label=\"empirical acc\")\n    plt.scatter(centers, confs, s=20, label=\"mean conf\", zorder=5)\n    plt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\"); plt.title(\"Reliability Diagram\")\n    plt.legend(); plt.tight_layout(); plt.savefig(out_path, dpi=160); plt.close()\n\n# ----------------------------\n# Safe CSV reading + numeric coercion (no deprecated args)\n# ----------------------------\ndef safe_read_csv(path: Path) -> pd.DataFrame:\n    if not path or not path.exists() or path.stat().st_size == 0:\n        return pd.DataFrame()\n    try:\n        return pd.read_csv(path)\n    except Exception:\n        pass\n        reader = csv.reader(f)\n        rows = list(reader)\n    if not rows:\n        return pd.DataFrame()\n    header = rows[0]; n = len(header)\n    fixed = []\n    for r in rows[1:]:\n        if len(r) == n:\n            fixed.append(r)\n        elif len(r) > n:\n            fixed.append(r[:n-1] + [\",\".join(r[n-1:])])\n        else:\n            fixed.append(r + [\"\"]*(n-len(r)))\n    return pd.DataFrame(fixed, columns=header)\n\ndef try_to_numeric(s: pd.Series) -> pd.Series:\n    # Prefer strict conversion; if it fails, fall back to coerce (produces NaN for bad rows)\n    try:\n        return pd.to_numeric(s, errors=\"raise\")\n    except Exception:\n        return pd.to_numeric(s, errors=\"coerce\")\n\n# ----------------------------\n# HPO & retrain summaries\n# ----------------------------\ndef summarize_hpo(hpo_csv: Path, out_dir: Path):\n    df = safe_read_csv(hpo_csv)\n    if df.empty:\n        return None\n    cols_pref = [\"lr\",\"weight_decay\",\"dropout\",\"label_smoothing\",\"batch_size\",\n                 \"best_val_acc\",\"test_acc\",\"best_epoch\"]\n    keep = [c for c in cols_pref if c in df.columns]\n    # convert numeric-ish columns safely (avoids errors='ignore' deprecation)\n    for c in keep:\n        if c in df.columns:\n            df[c] = try_to_numeric(df[c]) if df[c].dtype == object else df[c]\n    # Rank & export\n    sort_cols = [c for c in [\"best_val_acc\",\"test_acc\"] if c in df.columns]\n    top10 = df.sort_values(sort_cols, ascending=False)[keep].head(10).copy()\n    top10.to_csv(out_dir / \"hpo_top10.csv\", index=False)\n    md = [\n        \"# HPO Top-10 (by val, then test)\",\n        \"\",\n        top10.to_markdown(index=False),\n        \"\",\n        f\"Total trials in CSV: {len(df)}\"\n    ]\n    (out_dir / \"hpo_summary.md\").write_text(\"\\n\".join(md))\n    return top10\n\ndef summarize_retrain(retrain_csv: Path, out_dir: Path):\n    df = safe_read_csv(retrain_csv)\n    if df.empty:\n        return None\n    for c in [\"val_mean\",\"test_mean\",\"val_std\",\"test_std\",\"n\",\"lr\",\"weight_decay\",\"dropout\",\"label_smoothing\",\"batch_size\"]:\n        if c in df.columns and df[c].dtype == object:\n            df[c] = try_to_numeric(df[c])\n    sort_cols = [c for c in [\"val_mean\",\"test_mean\"] if c in df.columns]\n    df_sorted = df.sort_values(sort_cols, ascending=False).reset_index(drop=True)\n    df_sorted.to_csv(out_dir / \"retrain_summary_sorted.csv\", index=False)\n    return df_sorted\n\n# ----------------------------\n# Main\n# ----------------------------\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True, help=\"Project root, e.g., /content/drive/MyDrive/Alzheimers\")\n    ap.add_argument(\"--checkpoint\", type=str, required=True, help=\"Path to final_best_resnet50.pt OR HPO best .pt (relative to proj_root)\")\n    ap.add_argument(\"--data_dir\", type=str, required=True, help=\"Test set dir (ImageFolder, relative to proj_root)\")\n    ap.add_argument(\"--hpo_csv\", type=str, default=\"\", help=\"Optional HPO CSV (relative to proj_root)\")\n    ap.add_argument(\"--retrain_csv\", type=str, default=\"\", help=\"Optional retrain summary CSV (relative to proj_root)\")\n    # Optional overrides (if omitted, infer where possible)\n    ap.add_argument(\"--dropout\", type=float, default=None, help=\"Override dropout p; head STYLE auto-inferred from checkpoint keys\")\n    ap.add_argument(\"--img_size\", type=int, default=None, help=\"Override image size; default 224 for ResNet50\")\n    ap.add_argument(\"--batch_size\", type=int, default=None, help=\"Override eval batch size; default 32 or inferred from checkpoint\")\n    args = ap.parse_args()\n\n    proj_root = Path(os.path.expanduser(args.proj_root))\n    ckpt_path = proj_root / args.checkpoint\n    test_dir  = proj_root / args.data_dir\n    hpo_csv   = proj_root / args.hpo_csv if args.hpo_csv else None\n    retr_csv  = proj_root / args.retrain_csv if args.retrain_csv else None\n\n    reports_dir = ckpt_path.parent / \"reports\"\n    reports_dir.mkdir(parents=True, exist_ok=True)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --- Load checkpoint & extract state_dict/params ---\n    payload = torch.load(ckpt_path, map_location=device)\n    if isinstance(payload, dict) and \"state_dict\" in payload:\n        state_dict = payload[\"state_dict\"]\n        ckpt_params = payload.get(\"params\", {})\n    else:\n        state_dict = payload  # raw state_dict\n        ckpt_params = {}\n\n    # Determine head style; choose dropout value\n    head_style = detect_head_style_from_state_dict(state_dict)\n    inferred_dropout = float(ckpt_params.get(\"dropout\")) if \"dropout\" in ckpt_params else None\n    dropout_p = args.dropout if args.dropout is not None else (inferred_dropout if inferred_dropout is not None else (0.3 if head_style == \"seq\" else 0.0))\n\n    # Infer batch size if present\n    inferred_bs = int(ckpt_params.get(\"batch_size\")) if \"batch_size\" in ckpt_params else None\n    img_size = args.img_size if args.img_size is not None else 224\n    eval_bs  = args.batch_size if args.batch_size is not None else (inferred_bs if inferred_bs is not None else 32)\n\n    print(f\"[final_eval] Head style: {head_style} | dropout_p={dropout_p} | img_size={img_size} | eval_batch_size={eval_bs}\")\n\n    # Dataset / loader\n    tf_eval = make_eval_tf(img_size)\n    test_set = datasets.ImageFolder(str(test_dir), transform=tf_eval)\n    class_names = test_set.classes\n    num_classes = len(class_names)\n    loader = torch.utils.data.DataLoader(\n        test_set, batch_size=eval_bs, shuffle=False,\n        num_workers=4, pin_memory=True, persistent_workers=True\n    )\n\n    # Model + weights (style must match keys)\n    model = build_resnet50_for_style(num_classes=num_classes, head_style=head_style, dropout_p=dropout_p).to(device)\n    model.load_state_dict(state_dict)\n\n    # Predict & metrics\n    probs, y_pred, y_true = predict(model, loader, device)\n    report_txt = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n    acc = accuracy_score(y_true, y_pred)\n\n    (reports_dir / \"classification_report.txt\").write_text(report_txt + f\"\\n\\nAccuracy: {acc:.6f}\\n\")\n    with open(reports_dir / \"metrics.json\", \"w\") as f:\n        json.dump({\"accuracy\": float(acc)}, f, indent=2)\n\n    plot_confusion_matrix(y_true, y_pred, class_names, reports_dir / \"confusion_matrix.png\")\n    roc_stats = plot_roc_curves(y_true, probs, class_names, reports_dir)\n    pr_stats  = plot_pr_curves(y_true, probs, class_names, reports_dir)\n\n    ece, bin_stats = expected_calibration_error(probs, y_true, n_bins=15)\n    plot_reliability(bin_stats, reports_dir / \"calibration_reliability.png\")\n    (reports_dir / \"ece.txt\").write_text(f\"ECE (15 bins): {ece:.6f}\\n\")\n\n    with open(reports_dir / \"metrics.json\", \"r\") as f:\n        m = json.load(f)\n    m.update({\"roc\": roc_stats, \"pr\": pr_stats, \"ece_15bins\": float(ece)})\n    with open(reports_dir / \"metrics.json\", \"w\") as f:\n        json.dump(m, f, indent=2)\n\n    # Optional: HPO & retrain summaries\n    top10 = summarize_hpo(hpo_csv, reports_dir) if hpo_csv else None\n    retr  = summarize_retrain(retr_csv, reports_dir) if retr_csv else None\n\n    # One-page report\n    lines = [\n        \"# Final Evaluation Report (ResNet50)\",\n        \"\",\n        f\"- **Checkpoint:** `{ckpt_path}`\",\n        f\"- **Test dir:** `{test_dir}`\",\n        f\"- **Image size:** {img_size}\",\n        f\"- **Eval batch size:** {eval_bs}\",\n        f\"- **Head style:** `{head_style}`\",\n        f\"- **Head dropout p (inactive in eval):** {dropout_p}\",\n        \"\",\n        \"## Overall Metrics\",\n        f\"- **Accuracy:** `{acc:.4f}`\",\n        f\"- **ROC AUC (micro):** `{m['roc']['auc_micro']:.4f}`  |  **ROC AUC (macro):** `{m['roc']['auc_macro']:.4f}`\",\n        f\"- **PR AP (micro):** `{m['pr']['ap_micro']:.4f}`  |  **PR AP (macro):** `{m['pr']['ap_macro']:.4f}`\",\n        f\"- **ECE (15 bins):** `{m['ece_15bins']:.4f}`\",\n        \"\",\n        \"## Plots\",\n        \"- Confusion matrix: `confusion_matrix.png`\",\n        \"- ROC curves: `roc_curves_perclass.png`, `roc_curves_micro.png`, `roc_curves_macro.png`\",\n        \"- PR curves: `pr_curves_perclass.png`, `pr_curves_micro.png`, `pr_curves_macro.png`\",\n        \"- Calibration: `calibration_reliability.png`\",\n        \"\",\n    ]\n    if top10 is not None:\n        lines += [\n            \"## HPO Summary\",\n            f\"- Source CSV: `{hpo_csv}`\",\n            \"- Top-10 table: `hpo_top10.csv`\",\n            \"\",\n            top10.to_markdown(index=False),\n            \"\"\n        ]\n    if retr is not None:\n        lines += [\n            \"## Retrain (mean Â± std) Summary\",\n            f\"- Source CSV: `{retr_csv}`\",\n            \"- Sorted summary: `retrain_summary_sorted.csv`\",\n            \"\",\n            retr.head(10).to_markdown(index=False),\n            \"\"\n        ]\n\n    (reports_dir / \"REPORT.md\").write_text(\"\\n\".join(lines))\n\n    print(f\"\\nSaved complete report package to: {reports_dir}\")\n    print(f\"Test accuracy: {acc:.4f}\")\n    if hpo_csv:  print(\"Included HPO top-10 summary.\")\n    if retr_csv: print(\"Included retrain meanÂ±std summary.\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
