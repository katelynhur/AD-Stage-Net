{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import sys\nsys.argv = [\"\", \"--proj_root\", \"/content/drive/MyDrive/Alzheimers\", \"--inputs\", \"Luke=Results/Model_Leaderboard_Singles_Luke/leaderboard.csv,Marco=Results/Model_Leaderboard_Singles_Marco/leaderboard.csv,Falah=Results/Model_Leaderboard_Singles_Falah/leaderboard.csv\", \"--out_dir\", \"Results/Combined_Singles_Leaderboard\", \"--join_on\", \"arch\", \"--top_n\", \"20\"]\n\n\"\"\"\nCombine multiple Singles leaderboards (each from 3-2_summarize_models_v2.py)\ninto one comparison table.\n\nInputs: one or more \"LABEL=path/to/leaderboard.csv\"\n\nOutputs under <proj_root>/<out_dir>/:\n- combined_singles.csv       # wide table: test_acc__LABEL columns\n- combined_singles.md        # markdown preview\n- combined_singles_long.csv  # tidy format: (arch, cfg_hash, dataset, test_acc)\n- Figures/combined_top_models.png  # grouped bars of top-N by mean across datasets\n\"\"\"\n\nimport os\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef short_label(s, max_len=28):\n    s = str(s)\n    return s if len(s) <= max_len else s[:max_len-1] + \"…\"\n\ndef read_lb(p):\n    df = pd.read_csv(p)\n    # normalize types we care about\n    for c in [\"test_acc\",\"best_val_acc\",\"img_size\",\"batch_size\",\"global_batch_size\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    for c in [\"arch\",\"cfg_hash\",\"name\",\"family\",\"source\",\"run_dir\"]:\n        if c not in df.columns:\n            df[c] = \"\"\n    return df\n\ndef dataframe_to_markdown(df: pd.DataFrame, max_rows=30) -> str:\n    try:\n        return df.head(max_rows).to_markdown(index=False)\n    except Exception:\n        return df.head(max_rows).to_string(index=False)\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True)\n    ap.add_argument(\n        \"--inputs\",\n        type=str,\n        required=True,\n        help='Comma-separated: \"Luke=Results/Model_Leaderboard_Singles_Luke/leaderboard.csv,'\n             'Marco=Results/Model_Leaderboard_Singles_Marco/leaderboard.csv,'\n             'Falah=Results/Model_Leaderboard_Singles_Falah/leaderboard.csv\"'\n    )\n    ap.add_argument(\"--out_dir\", type=str, default=\"Results/Combined_Singles_Leaderboard\")\n    ap.add_argument(\"--join_on\", type=str, default=\"arch\", choices=[\"arch\", \"arch_cfg\"],\n                    help='Join key: \"arch\" (default) or \"arch_cfg\" (use both arch and cfg_hash).')\n    ap.add_argument(\"--top_n\", type=int, default=20, help=\"For the grouped bar chart.\")\n    args = ap.parse_args()\n\n    proj = Path(os.path.expanduser(args.proj_root)).resolve()\n    out_dir = (proj / args.out_dir).resolve()\n    fig_dir = out_dir / \"Figures\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    fig_dir.mkdir(parents=True, exist_ok=True)\n\n    # Parse inputs\n    items = [s.strip() for s in args.inputs.split(\",\") if s.strip()]\n    pairs = []\n    for it in items:\n        if \"=\" not in it:\n            raise ValueError(f'Bad --inputs item (missing \"=\"): {it}')\n        label, rel = it.split(\"=\", 1)\n        label = label.strip()\n        p = (proj / rel.strip()).resolve()\n        if not p.exists():\n            raise FileNotFoundError(f\"{label}: file not found: {p}\")\n        pairs.append((label, p))\n\n    # Load and prepare each leaderboard\n    key_cols = [\"arch\"] if args.join_on == \"arch\" else [\"arch\", \"cfg_hash\"]\n    wide = None\n    meta_cols = [\"arch\", \"cfg_hash\", \"family\", \"name\"]  # carry from the first table, when available\n    all_longs = []\n\n    for idx, (label, path) in enumerate(pairs):\n        df = read_lb(path)\n\n        # If multiple rows per arch (unlikely for single_best_only), keep the top test_acc per key\n        df = df.sort_values(\"test_acc\", ascending=False)\n        df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n\n        # Make a slim view for merging\n        slim_cols = list(dict.fromkeys(key_cols + [\"test_acc\", \"name\", \"family\"]))\n        slim = df[slim_cols].copy()\n        slim = slim.rename(columns={\"test_acc\": f\"test_acc__{label}\"})\n\n        # Long/tidy for plotting across datasets later\n        long = df[key_cols + [\"test_acc\"]].copy()\n        long[\"dataset\"] = label\n        all_longs.append(long)\n\n        if wide is None:\n            # Start with the first table and keep some meta columns\n            base_cols = list(dict.fromkeys(key_cols + meta_cols))\n            base = df.copy()\n            for c in base_cols:\n                if c not in base.columns:\n                    base[c] = \"\"\n            wide = base[base_cols].drop_duplicates(subset=key_cols, keep=\"first\").copy()\n            wide = wide.merge(slim[key_cols + [f\"test_acc__{label}\"]], on=key_cols, how=\"left\")\n        else:\n            wide = wide.merge(slim[key_cols + [f\"test_acc__{label}\"]], on=key_cols, how=\"outer\")\n\n    # Compute mean across datasets (for sorting/plotting)\n    dataset_cols = [c for c in wide.columns if c.startswith(\"test_acc__\")]\n    wide[\"mean_test_acc\"] = wide[dataset_cols].mean(axis=1, skipna=True)\n\n    # Sort by mean desc\n    wide = wide.sort_values(\"mean_test_acc\", ascending=False).reset_index(drop=True)\n\n    # If name is missing, synthesize it from arch (+ short cfg)\n    if \"name\" in wide.columns:\n        needs_name = wide[\"name\"].isna() | (wide[\"name\"].astype(str).str.len() == 0)\n        if \"cfg_hash\" in wide.columns:\n            wide.loc[needs_name, \"name\"] = wide.loc[needs_name].apply(\n                lambda r: f\"{r.get('arch','?')} ({str(r.get('cfg_hash',''))[:6]})\", axis=1\n            )\n        else:\n            wide.loc[needs_name, \"name\"] = wide.loc[needs_name, \"arch\"]\n\n    # Save wide + md\n    (out_dir / \"combined_singles.csv\").write_text(wide.to_csv(index=False))\n    (out_dir / \"combined_singles.md\").write_text(dataframe_to_markdown(wide, max_rows=60))\n\n    # Save long/tidy\n    long_df = pd.concat(all_longs, ignore_index=True)\n    long_df.to_csv(out_dir / \"combined_singles_long.csv\", index=False)\n\n    # -------- Plot: grouped bars for top-N by mean_test_acc --------\n    top = wide.head(args.top_n).copy()\n    if not top.empty and len(dataset_cols) >= 2:\n        x = np.arange(len(top))\n        width = max(0.8 / len(dataset_cols), 0.15)\n\n        plt.figure(figsize=(max(10, 1.2 * len(top)), 6))\n        for i, col in enumerate(dataset_cols):\n            vals = top[col].values\n            plt.bar(x + i * width, vals, width=width, label=col.replace(\"test_acc__\", \"\"))\n\n            # annotate bars\n            for j, v in enumerate(vals):\n                if pd.notna(v):\n                    plt.text(x[j] + i * width, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n\n        xt = top[\"name\"] if \"name\" in top.columns else top[\"arch\"]\n        plt.xticks(x + (len(dataset_cols) - 1) * width / 2, [short_label(t) for t in xt], rotation=45, ha=\"right\")\n        plt.ylabel(\"Test Accuracy\")\n        plt.title(f\"Top {args.top_n} Models — cross-dataset comparison\")\n        plt.legend(title=\"Dataset\")\n        plt.tight_layout()\n        plt.savefig(fig_dir / \"combined_top_models.png\", dpi=200)\n        plt.close()\n\n    print(\"Wrote:\")\n    print(f\"- {out_dir / 'combined_singles.csv'}\")\n    print(f\"- {out_dir / 'combined_singles.md'}\")\n    print(f\"- {out_dir / 'combined_singles_long.csv'}\")\n    print(f\"- {fig_dir / 'combined_top_models.png'}\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
