{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import sys\nsys.argv = [\"\", \"--results_dir\", \"/content/drive/MyDrive/Alzheimers/Results/HP_Search_Luke_DDP\", \"--csv\", \"resnet50_luke_small_search_ddp.csv\", \"--topk\", \"3\"]\n\n\"\"\"\nRetrain top-K configs (from HPO CSV) with multiple seeds and summarize results.\n\nNEW IN THIS VERSION\n- Creates best_params.json immediately from the HPO CSV (pre-retrain best),\n  so other scripts can reuse the best hyperparameters even if you skip retraining.\n\nPipeline\n- Reads HPO CSV (from small_search_resnet50_luke_ddp.py)\n- Writes best_params.json (top row by best_val_acc then test_acc)\n- Selects top-K unique param sets\n- Retrains each config for multiple seeds (default 3 via --seeds 202 303 404)\n- Writes:\n  - retrain_per_seed_results.csv\n  - retrain_summary_mean_std.csv\n  - final_best_resnet50.pt (best single-seed checkpoint among retrains)\n  - best_retrained.json (best-by-mean-across-seeds hyperparams & stats)\n\nGPU behavior\n- Auto-detect A800 GPUs (via nvidia-smi) and re-exec with CUDA_VISIBLE_DEVICES\n  restricted to those. If none found or nvidia-smi unavailable, leave GPUs as-is.\n- DataParallel uses all visible GPUs (no T1000 imbalance warning if hidden).\n\nPaths assume LukeChugh dataset:\n  /content/drive/MyDrive/Alzheimers/Data/Kaggle_LukeChugh_Best_Alzheimers_MRI/{train,test}\n\nUsage:\n  python retrain_topk_with_seeds.py \\\n     --results_dir /content/drive/MyDrive/Alzheimers/Results/HP_Search_Luke_DDP \\\n     --csv resnet50_luke_small_search_ddp.csv \\\n     --topk 3 \\\n     --seeds 202 303 404\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport random\nimport argparse\nimport subprocess\nimport csv\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# Early arg parse (for GPU setup)\n# ----------------------------\nap = argparse.ArgumentParser()\nap.add_argument(\"--results_dir\", type=str, required=True, help=\"Directory with HPO CSV & where to save outputs\")\nap.add_argument(\"--csv\", type=str, required=True, help=\"HPO CSV filename\")\nap.add_argument(\"--topk\", type=int, default=3, help=\"Top-K configs to retrain\")\nap.add_argument(\"--seeds\", type=int, nargs=\"+\", default=[202, 303, 404], help=\"Seeds to use for retraining\")\nap.add_argument(\"--gpus\", type=str, default=\"auto\",\n                help='GPU selection: \"auto\" (pick A800s), \"\" (no override), or e.g. \"0,2,3\"')\nargs, _ = ap.parse_known_args()\n\n# ----------------------------\n# Auto GPU selection (A800) + re-exec once\n# ----------------------------\ndef detect_gpu_indices_by_name(match_any: List[str]) -> List[str]:\n    try:\n        out = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,name\", \"--format=csv,noheader\"],\n            stderr=subprocess.STDOUT, text=True\n        )\n    except Exception:\n        return []\n    picks = []\n    for line in out.strip().splitlines():\n        parts = [p.strip() for p in line.split(\",\")]\n        if len(parts) >= 2:\n            idx, name = parts[0], \",\".join(parts[1:]).strip()\n            if any(tok.lower() in name.lower() for tok in match_any):\n                picks.append(idx)\n    return picks\n\ndef ensure_cuda_visibility():\n    if os.environ.get(\"_CUDA_VIS_SET\") == \"1\":\n        return\n    os.environ.setdefault(\"CUDA_DEVICE_ORDER\", \"PCI_BUS_ID\")\n\n    # Respect explicit --gpus (including empty string meaning \"no override\")\n    if args.gpus != \"auto\":\n        if args.gpus.strip():\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus.strip()\n        os.environ[\"_CUDA_VIS_SET\"] = \"1\"\n        os.execvpe(sys.executable, [sys.executable] + sys.argv, os.environ)\n\n    # Auto: prefer A800s if found\n    if \"CUDA_VISIBLE_DEVICES\" not in os.environ or os.environ[\"CUDA_VISIBLE_DEVICES\"] == \"\":\n        picks = detect_gpu_indices_by_name([\"A800\"])\n        if picks:\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(picks)\n            os.environ[\"_CUDA_VIS_SET\"] = \"1\"\n            os.execvpe(sys.executable, [sys.executable] + sys.argv, os.environ)\n\nensure_cuda_visibility()\n\n# Only now is it safe to import torch CUDA bits\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\nfrom torchvision.transforms import functional as F, InterpolationMode\n\n# ----------------------------\n# Stable ordering + device\n# ----------------------------\nos.environ.setdefault(\"CUDA_DEVICE_ORDER\", \"PCI_BUS_ID\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ----------------------------\n# CSV robustness\n# ----------------------------\ndef safe_read_results_csv(csv_path: Path) -> pd.DataFrame:\n    if not csv_path.exists() or csv_path.stat().st_size == 0:\n        return pd.DataFrame()\n        reader = csv.reader(f)\n        rows = list(reader)\n    if not rows:\n        return pd.DataFrame()\n    header = rows[0]; n = len(header)\n    fixed = []; bad = 0\n    for r in rows[1:]:\n        if len(r) == n:\n            fixed.append(r)\n        elif len(r) > n:\n            fixed.append(r[:n-1] + [\",\".join(r[n-1:])]); bad += 1\n        else:\n            fixed.append(r + [\"\"]*(n-len(r))); bad += 1\n    if bad:\n        print(f\"[safe_read_results_csv] Repaired {bad} malformed rows in {csv_path.name}\")\n    return pd.DataFrame(fixed, columns=header)\n\n\n# ----------------------------\n# Transforms (Pad -> Resize 224)\n# ----------------------------\nclass PadToSquare:\n    def __call__(self, img):\n        w, h = F.get_image_size(img)\n        s = max(w, h)\n        pad_l = (s - w) // 2\n        pad_r = s - w - pad_l\n        pad_t = (s - h) // 2\n        pad_b = s - h - pad_t\n        return F.pad(img, [pad_l, pad_t, pad_r, pad_b], fill=0)\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD  = [0.229, 0.224, 0.225]\n\ntf_train = transforms.Compose([\n    PadToSquare(),\n    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC, antialias=True),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomRotation(10, interpolation=InterpolationMode.BILINEAR),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n])\n\ntf_eval = transforms.Compose([\n    PadToSquare(),\n    transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC, antialias=True),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n])\n\n\n# ----------------------------\n# Utils\n# ----------------------------\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef build_resnet50(num_classes: int, dropout: float):\n    m = models.resnet50(weights=\"IMAGENET1K_V1\")\n    in_dim = m.fc.in_features\n    if dropout and dropout > 0:\n        m.fc = nn.Sequential(nn.Dropout(p=float(dropout)), nn.Linear(in_dim, num_classes))\n    else:\n        m.fc = nn.Linear(in_dim, num_classes)\n    return m\n\n@torch.no_grad()\ndef evaluate(model, loader, dev):\n    model.eval()\n    correct = total = 0\n    for xb, yb in loader:\n        xb, yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n        pred = model(xb).argmax(1)\n        correct += (pred == yb).sum().item()\n        total += yb.size(0)\n    return correct / total if total else 0.0\n\ndef fmt_float(x: float) -> str:\n    return f\"{float(x):.8f}\".rstrip(\"0\").rstrip(\".\")\n\ndef param_key_from_dict(p: Dict[str, Any]) -> str:\n    # Stable compact key for filenames (avoids scientific notation)\n    order = [\"lr\", \"weight_decay\", \"dropout\", \"label_smoothing\", \"batch_size\"]\n    parts = []\n    for k in order:\n        v = p[k]\n        if isinstance(v, (float, np.floating)):\n            parts.append(f\"{k}-{fmt_float(float(v))}\")\n        else:\n            parts.append(f\"{k}-{int(v) if k=='batch_size' else v}\")\n    return \"__\".join(parts)\n\n\n# ----------------------------\n# Data split & loaders\n# ----------------------------\ndef make_split_loaders(train_dir: Path, test_dir: Path, batch_size: int, seed_for_split: int = 42):\n    full_train_aug  = datasets.ImageFolder(str(train_dir), transform=tf_train)\n    full_train_eval = datasets.ImageFolder(str(train_dir), transform=tf_eval)\n    test_set        = datasets.ImageFolder(str(test_dir),  transform=tf_eval)\n\n    targets = [y for _, y in full_train_aug.samples]\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed_for_split)\n    train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n\n    train_ds = Subset(full_train_aug, train_idx)\n    val_ds   = Subset(full_train_eval, val_idx)\n\n    num_classes = len(full_train_aug.classes)\n    class_names = full_train_aug.classes\n\n    common = dict(batch_size=batch_size, num_workers=4, pin_memory=True, persistent_workers=True)\n    train_loader = DataLoader(train_ds, shuffle=True,  **common)\n    val_loader   = DataLoader(val_ds,   shuffle=False, **common)\n    test_loader  = DataLoader(test_set, shuffle=False, **common)\n\n    train_labels = [full_train_aug.samples[i][1] for i in train_idx]\n    class_counts = np.bincount(train_labels, minlength=num_classes)\n    total = class_counts.sum()\n    weights = total / (class_counts + 1e-12)\n    class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n\n    return train_loader, val_loader, test_loader, num_classes, class_names, class_weights\n\n\n# ----------------------------\n# Train one config across seeds\n# ----------------------------\ndef train_one_run(params: dict, seeds: list, proj_root: Path, results_dir: Path):\n    train_dir = proj_root / \"Data\" / \"Kaggle_LukeChugh_Best_Alzheimers_MRI\" / \"train\"\n    test_dir  = proj_root / \"Data\" / \"Kaggle_LukeChugh_Best_Alzheimers_MRI\" / \"test\"\n\n    SPLIT_SEED = 42  # fixed split across seeds\n\n    best_seed_ckpt = None\n    best_seed_val  = -1.0\n    per_seed_rows  = []\n\n    for seed in seeds:\n        set_seed(int(seed))\n\n        train_loader, val_loader, test_loader, num_classes, _, class_weights = make_split_loaders(\n            train_dir, test_dir, batch_size=int(params[\"batch_size\"]), seed_for_split=SPLIT_SEED\n        )\n\n        model = build_resnet50(num_classes, float(params[\"dropout\"])).to(device)\n        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)  # uses all visible GPUs\n\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=float(params[\"label_smoothing\"]))\n        optimizer = optim.AdamW(model.parameters(), lr=float(params[\"lr\"]), weight_decay=float(params[\"weight_decay\"]))\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3, min_lr=1e-6)\n\n        max_epochs = 30\n        patience   = 10\n        wait = 0\n        best_val, best_epoch = -1.0, -1\n\n        pkey = param_key_from_dict(params)\n        ckpt_path = results_dir / f\"retrain_ckpt_{pkey}__seed-{int(seed)}.pt\"\n\n        for epoch in range(1, max_epochs + 1):\n            model.train()\n            for xb, yb in train_loader:\n                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n                optimizer.zero_grad(set_to_none=True)\n                loss = criterion(model(xb), yb)\n                loss.backward()\n                optimizer.step()\n\n            val_acc = evaluate(model, val_loader, device)\n            scheduler.step(val_acc)\n\n            if val_acc > best_val + 1e-6:\n                best_val, best_epoch = val_acc, epoch\n                wait = 0\n                core = model.module if isinstance(model, nn.DataParallel) else model\n                torch.save({\n                    \"state_dict\": core.state_dict(),\n                    \"params\": dict(params),\n                    \"seed\": int(seed),\n                    \"epoch\": int(epoch),\n                    \"val_acc\": float(val_acc)\n                }, ckpt_path)\n            else:\n                wait += 1\n                if wait >= patience:\n                    break\n\n        # Test best weights\n        payload = torch.load(ckpt_path, map_location=device)\n        core = model.module if isinstance(model, nn.DataParallel) else model\n        core.load_state_dict(payload[\"state_dict\"])\n        test_acc = evaluate(core, test_loader, device)\n\n        per_seed_rows.append({\n            **params,\n            \"seed\": int(seed),\n            \"best_val_acc\": round(float(payload[\"val_acc\"]), 6),\n            \"best_epoch\": int(payload[\"epoch\"]),\n            \"test_acc\": round(float(test_acc), 6),\n            \"ckpt_path\": str(ckpt_path)\n        })\n\n        if payload[\"val_acc\"] > best_seed_val:\n            best_seed_val = float(payload[\"val_acc\"])\n            best_seed_ckpt = ckpt_path\n\n    return per_seed_rows, best_seed_ckpt\n\n\n# ----------------------------\n# Main\n# ----------------------------\ndef main():\n    results_dir = Path(args.results_dir).expanduser()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    csv_path = results_dir / args.csv\n    if not csv_path.exists():\n        print(f\"CSV not found: {csv_path}\")\n        sys.exit(1)\n\n    print(f\"CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES', '') or '<all visible>'}\")\n    if torch.cuda.is_available():\n        n = torch.cuda.device_count()\n        print(f\"Using {n} visible GPU(s):\")\n        for i in range(n):\n            print(f\"  cuda:{i} -> {torch.cuda.get_device_name(i)}\")\n    else:\n        print(\"CUDA not available; using CPU.\")\n\n    # Read & sort HPO CSV\n    df = safe_read_results_csv(csv_path)\n    if df.empty:\n        print(\"HPO CSV is empty; nothing to retrain.\")\n        sys.exit(0)\n\n    # Coerce numerics\n    for c in [\"lr\",\"weight_decay\",\"dropout\",\"label_smoothing\",\"batch_size\",\"best_val_acc\",\"test_acc\",\"best_epoch\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n    df_sorted = df.sort_values([\"best_val_acc\", \"test_acc\"], ascending=False, na_position=\"last\").reset_index(drop=True)\n\n    # ---- Write best_params.json from the top row of HPO CSV ----\n    best_row = df_sorted.iloc[0].to_dict()\n    best_params = {\n        \"arch\": \"ResNet50\",\n        \"lr\": float(best_row[\"lr\"]),\n        \"weight_decay\": float(best_row[\"weight_decay\"]),\n        \"dropout\": float(best_row[\"dropout\"]),\n        \"label_smoothing\": float(best_row[\"label_smoothing\"]),\n        \"batch_size\": int(best_row[\"batch_size\"]),\n        \"best_val_acc\": float(best_row[\"best_val_acc\"]),\n        \"best_epoch\": int(best_row[\"best_epoch\"]) if not pd.isna(best_row.get(\"best_epoch\", np.nan)) else None,\n        \"test_acc\": float(best_row[\"test_acc\"]),\n        \"checkpoint\": str(best_row[\"ckpt_path\"]) if \"ckpt_path\" in best_row and pd.notna(best_row[\"ckpt_path\"]) else \"\",\n        \"source\": \"HPO_CSV\"\n    }\n    (results_dir / \"best_params.json\").write_text(json.dumps(best_params, indent=2))\n    print(f\"Wrote HPO best params to: {results_dir / 'best_params.json'}\")\n\n    # Select top-K UNIQUE param sets\n    param_cols = [\"lr\", \"weight_decay\", \"dropout\", \"label_smoothing\", \"batch_size\"]\n    topk_df = df_sorted[param_cols].drop_duplicates().head(args.topk).reset_index(drop=True)\n    print(f\"Selected top-{len(topk_df)} unique configs for retraining:\\n{topk_df}\")\n\n    proj_root = Path(\"/content/drive/MyDrive/Alzheimers\")\n    per_seed_out = results_dir / \"retrain_per_seed_results.csv\"\n    summary_out  = results_dir / \"retrain_summary_mean_std.csv\"\n    overall_best = results_dir / \"final_best_resnet50.pt\"\n\n    all_rows = []\n    best_overall_val = -1.0\n    best_overall_ckpt = None\n\n    for i, row in topk_df.iterrows():\n        params = {\n            \"lr\": float(row[\"lr\"]),\n            \"weight_decay\": float(row[\"weight_decay\"]),\n            \"dropout\": float(row[\"dropout\"]),\n            \"label_smoothing\": float(row[\"label_smoothing\"]),\n            \"batch_size\": int(row[\"batch_size\"]),\n        }\n        print(f\"\\n=== Retraining config {i+1}/{len(topk_df)}: {params} ===\")\n        per_seed_rows, best_ckpt_for_config = train_one_run(params, args.seeds, proj_root, results_dir)\n        all_rows.extend(per_seed_rows)\n\n        # Track best across configs by highest validation among best seeds\n        best_cfg_val = max(r[\"best_val_acc\"] for r in per_seed_rows)\n        if best_cfg_val > best_overall_val:\n            best_overall_val = best_cfg_val\n            best_overall_ckpt = best_ckpt_for_config\n\n    # Save per-seed results\n    df_seeds = pd.DataFrame(all_rows)\n    df_seeds.to_csv(per_seed_out, index=False,\n                    quoting=csv.QUOTE_MINIMAL, escapechar=\"\\\\\", line_terminator=\"\\n\")\n    print(f\"\\nSaved per-seed results: {per_seed_out}\")\n\n    # Summarize mean ± std per config\n    group_cols = param_cols\n    agg = df_seeds.groupby(group_cols, dropna=False).agg(\n        val_mean=(\"best_val_acc\", \"mean\"),\n        val_std =(\"best_val_acc\", \"std\"),\n        test_mean=(\"test_acc\", \"mean\"),\n        test_std =(\"test_acc\", \"std\"),\n        n=(\"seed\", \"count\")\n    ).reset_index()\n\n    for c in [\"val_mean\",\"val_std\",\"test_mean\",\"test_std\"]:\n        agg[c] = agg[c].astype(float).round(6)\n\n    agg.to_csv(summary_out, index=False,\n               quoting=csv.QUOTE_MINIMAL, escapechar=\"\\\\\", line_terminator=\"\\n\")\n    print(f\"Saved summary (mean ± std): {summary_out}\")\n\n    # Copy overall best single-seed checkpoint\n    if best_overall_ckpt and Path(best_overall_ckpt).exists():\n        Path(best_overall_ckpt).replace(overall_best)\n        print(f\"Saved overall best checkpoint (single-seed) to: {overall_best}\")\n    else:\n        print(\"No overall best checkpoint found to copy.\")\n\n    # Also write best_retrained.json by mean across seeds\n    agg_sorted = agg.sort_values([\"val_mean\",\"test_mean\"], ascending=False).reset_index(drop=True)\n    best_mean = agg_sorted.iloc[0].to_dict()\n    best_retrained = {\n        \"arch\": \"ResNet50\",\n        \"lr\": float(best_mean[\"lr\"]),\n        \"weight_decay\": float(best_mean[\"weight_decay\"]),\n        \"dropout\": float(best_mean[\"dropout\"]),\n        \"label_smoothing\": float(best_mean[\"label_smoothing\"]),\n        \"batch_size\": int(best_mean[\"batch_size\"]),\n        \"val_mean\": float(best_mean[\"val_mean\"]),\n        \"val_std\": float(best_mean[\"val_std\"]) if not np.isnan(best_mean[\"val_std\"]) else None,\n        \"test_mean\": float(best_mean[\"test_mean\"]),\n        \"test_std\": float(best_mean[\"test_std\"]) if not np.isnan(best_mean[\"test_std\"]) else None,\n        \"n_seeds\": int(best_mean[\"n\"]),\n        \"checkpoint_single_seed_best\": str(overall_best),\n        \"source\": \"RETRAIN_MEAN_ACROSS_SEEDS\"\n    }\n    (results_dir / \"best_retrained.json\").write_text(json.dumps(best_retrained, indent=2))\n    print(f\"Wrote retrain best (mean across seeds) to: {results_dir / 'best_retrained.json'}\")\n\n    print(\"\\n=== Top configs (mean ± std) ===\")\n    print(agg_sorted.head(10))\n\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
