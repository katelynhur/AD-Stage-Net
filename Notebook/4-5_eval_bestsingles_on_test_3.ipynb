{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Install required packages and mount Google Drive\n!pip install -q torch torchvision pandas numpy scikit-learn\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import sys\nsys.argv = [\"\", \"--proj_root\", \"/content/drive/MyDrive/Alzheimers\", \"--weights_root\", \"Results/BestSingles\", \"--test_dir\", \"Data/HuggingFace_Falah_Alzheimer_MRI/test\", \"--out_subdir\", \"Results/BestSingles_Falah\", \"--sweep_csv\", \"Results/Singles_Luke/arch_sweep_results.csv\"]\n\n\"\"\"\nEvaluate a folder of .pt weights (BestSingles) on a separate ImageFolder test set.\n\nDefaults to STRICT loading for bit-for-bit identical accuracy versus the arch sweep.\nIf strict load fails (usually head-shape mismatch), the script reconstructs the\ndropout-wrapped head to match training, moves it to GPU, and retries strict load.\nYou can allow partial loads explicitly with --allow_partial_load 1.\n\nExample:\n  # Marco test set → Results/BestSingles_Marco\n  python 3-4_eval_bestsingles_on_test.py \\\n    --proj_root /content/drive/MyDrive/Alzheimers \\\n    --weights_root Results/BestSingles \\\n    --test_dir Data/Kaggle_MarcoPinamonti_Alzheimers_MRI/test \\\n    --out_subdir Results/BestSingles_Marco \\\n    --sweep_csv Results/Singles_Luke/arch_sweep_results.csv\n\"\"\"\n\nimport os, json, argparse\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, models, transforms\nfrom torchvision.transforms import InterpolationMode\nfrom torchvision.transforms import functional as F\nimport pandas as pd\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD  = [0.229, 0.224, 0.225]\n\n# -------------------\n# Transforms\n# -------------------\ndef make_eval_transform(arch_name: str):\n    size = 299 if arch_name.lower().startswith(\"inception\") else 224\n    class PadToSquare:\n        def __call__(self, img):\n            w, h = F.get_image_size(img)\n            s = max(w, h)\n            pad_l = (s - w) // 2\n            pad_r = s - w - pad_l\n            pad_t = (s - h) // 2\n            pad_b = s - h - pad_t\n            return F.pad(img, [pad_l, pad_t, pad_r, pad_b], fill=0)\n    tf_eval = transforms.Compose([\n        PadToSquare(),\n        transforms.Resize((size, size), interpolation=InterpolationMode.BICUBIC, antialias=True),\n        transforms.ToTensor(),\n        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n    ])\n    return tf_eval, size\n\n# -------------------\n# Model builders\n# -------------------\ndef base_build_model(arch: str, num_classes: int):\n    \"\"\"\n    Build backbone with a plain Linear head (unless torchvision default is Sequential).\n    Returns (model, head_attr_name).\n    \"\"\"\n    a = arch.strip()\n    if a == \"InceptionV3\":\n        m = models.inception_v3(weights=\"IMAGENET1K_V1\", aux_logits=True)\n        in_dim = m.fc.in_features\n        m.fc = nn.Linear(in_dim, num_classes)\n        return m, \"fc\"\n\n    if a == \"ResNet50\":\n        m = models.resnet50(weights=\"IMAGENET1K_V1\"); in_dim = m.fc.in_features; m.fc = nn.Linear(in_dim, num_classes); return m, \"fc\"\n    if a == \"ResNet101\":\n        m = models.resnet101(weights=\"IMAGENET1K_V1\"); in_dim = m.fc.in_features; m.fc = nn.Linear(in_dim, num_classes); return m, \"fc\"\n    if a == \"ResNet152\":\n        m = models.resnet152(weights=\"IMAGENET1K_V1\"); in_dim = m.fc.in_features; m.fc = nn.Linear(in_dim, num_classes); return m, \"fc\"\n\n    if a == \"DenseNet121\":\n        m = models.densenet121(weights=\"IMAGENET1K_V1\"); in_dim = m.classifier.in_features; m.classifier = nn.Linear(in_dim, num_classes); return m, \"classifier\"\n    if a == \"DenseNet161\":\n        m = models.densenet161(weights=\"IMAGENET1K_V1\"); in_dim = m.classifier.in_features; m.classifier = nn.Linear(in_dim, num_classes); return m, \"classifier\"\n    if a == \"DenseNet169\":\n        m = models.densenet169(weights=\"IMAGENET1K_V1\"); in_dim = m.classifier.in_features; m.classifier = nn.Linear(in_dim, num_classes); return m, \"classifier\"\n    if a == \"DenseNet201\":\n        m = models.densenet201(weights=\"IMAGENET1K_V1\"); in_dim = m.classifier.in_features; m.classifier = nn.Linear(in_dim, num_classes); return m, \"classifier\"\n\n    if a == \"EffNetB0\":\n        # torchvision default head is Sequential([Dropout, Linear])\n        m = models.efficientnet_b0(weights=\"IMAGENET1K_V1\")\n        in_dim = m.classifier[1].in_features\n        m.classifier[1] = nn.Linear(in_dim, num_classes)\n        return m, \"classifier\"\n\n    if a == \"MobileNetV2\":\n        m = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n        in_dim = m.classifier[-1].in_features\n        m.classifier[-1] = nn.Linear(in_dim, num_classes)\n        return m, \"classifier\"\n\n    if a == \"MobileNetV3_L\":\n        m = models.mobilenet_v3_large(weights=\"IMAGENET1K_V1\")\n        in_dim = m.classifier[-1].in_features\n        m.classifier[-1] = nn.Linear(in_dim, num_classes)\n        return m, \"classifier\"\n\n    if a == \"ResNeXt50_32x4d\":\n        m = models.resnext50_32x4d(weights=\"IMAGENET1K_V1\"); in_dim = m.fc.in_features; m.fc = nn.Linear(in_dim, num_classes); return m, \"fc\"\n    if a == \"ResNeXt101_32x8d\":\n        m = models.resnext101_32x8d(weights=\"IMAGENET1K_V1\"); in_dim = m.fc.in_features; m.fc = nn.Linear(in_dim, num_classes); return m, \"fc\"\n\n    if a == \"VGG16\":\n        m = models.vgg16_bn(weights=\"IMAGENET1K_V1\"); in_dim = m.classifier[-1].in_features; m.classifier[-1] = nn.Linear(in_dim, num_classes); return m, \"classifier\"\n\n    if a == \"CNN_Small\":\n        class YourSmallCNN(nn.Module):\n            def __init__(self, num_classes: int):\n                super().__init__()\n                self.features = nn.Sequential(\n                    nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n                    nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n                    nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n                )\n                self.classifier = nn.Linear(128, num_classes)\n            def forward(self, x):\n                z = self.features(x); z = z.view(z.size(0), -1); return self.classifier(z)\n        return YourSmallCNN(num_classes), \"classifier\"\n\n    raise ValueError(f\"Unknown arch: {arch}\")\n\ndef add_head_dropout(model, head_name, p, num_classes):\n    \"\"\"Wrap final Linear into Sequential(Dropout(p), Linear) if not already.\"\"\"\n    if p is None or p <= 0:\n        return model\n    layer = getattr(model, head_name)\n    # Plain Linear → wrap\n    if isinstance(layer, nn.Linear):\n        in_dim = layer.in_features\n        setattr(model, head_name, nn.Sequential(nn.Dropout(p), nn.Linear(in_dim, num_classes)))\n        return model\n    # Sequential → ensure Dropout directly before last Linear\n    if isinstance(layer, nn.Sequential):\n        new_seq = list(layer)\n        last_lin_idx = None\n        for i in reversed(range(len(new_seq))):\n            if isinstance(new_seq[i], nn.Linear):\n                last_lin_idx = i\n                break\n        if last_lin_idx is not None:\n            in_dim = new_seq[last_lin_idx].in_features\n            new_seq[last_lin_idx] = nn.Linear(in_dim, num_classes)\n            if last_lin_idx == 0 or not isinstance(new_seq[last_lin_idx - 1], nn.Dropout):\n                new_seq.insert(last_lin_idx, nn.Dropout(p))\n            setattr(model, head_name, nn.Sequential(*new_seq))\n            return model\n    return model\n\ndef state_uses_sequential_head(state_dict, head_name):\n    \"\"\"\n    Heuristic: if saved keys look like 'fc.1.weight' or 'classifier.1.weight', the saved model had a Sequential head.\n    \"\"\"\n    prefix = f\"{head_name}.\"\n    for k in state_dict.keys():\n        if k.startswith(prefix + \"1.weight\") or k.startswith(prefix + \"1.bias\"):\n            return True\n    return False\n\n# -------------------\n# Utils\n# -------------------\ndef infer_arch_and_params(pt_path: Path):\n    \"\"\"\n    Prefer a params.json next to the weights. Returns (arch_name, params_dict_or_empty).\n    \"\"\"\n    pjson = pt_path.with_name(\"params.json\")\n    if pjson.exists():\n        try:\n            blob = json.loads(pjson.read_text())\n            if \"arch\" in blob and blob[\"arch\"]:\n                return str(blob[\"arch\"]), blob\n        except Exception:\n            pass\n    # Fallback: guess from filename\n    name = pt_path.name\n    candidates = [\n        \"InceptionV3\",\"ResNeXt101_32x8d\",\"ResNeXt50_32x4d\",\"ResNet152\",\"ResNet101\",\"ResNet50\",\n        \"DenseNet201\",\"DenseNet169\",\"DenseNet161\",\"DenseNet121\",\n        \"EffNetB0\",\"MobileNetV3_L\",\"MobileNetV2\",\"VGG16\",\"CNN_Small\"\n    ]\n    for c in candidates:\n        if c.lower().replace(\"_\",\"\") in name.lower().replace(\"_\",\"\"):\n            return c, {}\n        if c.lower() in name.lower():\n            return c, {}\n    return None, {}\n\ndef pick_dropout_for_arch(arch: str, params_blob: dict, sweep_csv_path: Path | None):\n    \"\"\"\n    Decide dropout to reconstruct the head:\n    1) params.json next to weights\n    2) best arch row in sweep CSV (by test_acc)\n    3) else None (if sequential head detected we'll use 0.3 as safe default)\n    \"\"\"\n    if params_blob and (\"dropout\" in params_blob) and params_blob[\"dropout\"] is not None:\n        try:\n            return float(params_blob[\"dropout\"])\n        except Exception:\n            pass\n    if sweep_csv_path and sweep_csv_path.exists():\n        try:\n            df = pd.read_csv(sweep_csv_path)\n            if \"arch\" in df.columns:\n                df_arch = df[df[\"arch\"] == arch].copy()\n                if not df_arch.empty:\n                    sort_cols = [c for c in [\"test_acc\",\"best_val_acc\"] if c in df_arch.columns]\n                    if sort_cols:\n                        df_arch = df_arch.sort_values(sort_cols, ascending=False)\n                    dp = df_arch.iloc[0].get(\"dropout\", None)\n                    if pd.notna(dp):\n                        return float(dp)\n        except Exception:\n            pass\n    return None\n\n@torch.no_grad()\ndef evaluate(model, loader, device, arch_name):\n    model.eval()\n    correct, total = 0, 0\n    ce = nn.CrossEntropyLoss()\n    loss_sum = 0.0\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n        out = model(xb)\n        if arch_name.lower().startswith(\"inception\") and isinstance(out, tuple):\n            out = out[0]\n        loss = ce(out, yb)\n        loss_sum += float(loss.item()) * yb.size(0)\n        pred = out.argmax(1)\n        correct += int((pred == yb).sum().item())\n        total += int(yb.size(0))\n    acc = correct / max(total, 1)\n    return loss_sum / max(total, 1), acc\n\ndef try_strict_load(model, state_dict) -> bool:\n    try:\n        model.load_state_dict(state_dict, strict=True)\n        return True\n    except Exception as e:\n        print(f\"[ERROR] Strict load failed: {e}\")\n        return False\n\ndef partial_load_state(model, state_dict):\n    model_keys = model.state_dict()\n    filtered = {k: v for k, v in state_dict.items()\n                if k in model_keys and model_keys[k].shape == v.shape}\n    missing = set(model_keys.keys()) - set(filtered.keys())\n    if missing:\n        print(f\"[WARN] Skipping {len(missing)} keys due to shape mismatch (likely classifier layer).\")\n    model.load_state_dict(filtered, strict=False)\n\n# -------------------\n# Main\n# -------------------\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--proj_root\", type=str, required=True)\n    ap.add_argument(\"--weights_root\", type=str, default=\"Results/BestSingles\")\n    ap.add_argument(\"--pattern\", type=str, default=\"*.pt\", help=\"Glob for weight files (recurses).\")\n    ap.add_argument(\"--test_dir\", type=str, required=True)\n    ap.add_argument(\"--out_subdir\", type=str, required=True)\n    ap.add_argument(\"--batch_size\", type=int, default=32)\n    ap.add_argument(\"--num_workers\", type=int, default=4)\n    ap.add_argument(\"--sweep_csv\", type=str, default=\"\", help=\"Optional path to arch_sweep_results.csv to pick dropout per-arch.\")\n    ap.add_argument(\"--allow_partial_load\", type=int, default=0,\n                    help=\"If 1, fall back to shape-matched partial load when strict fails. Default 0 = strict only.\")\n    args = ap.parse_args()\n\n    proj = Path(os.path.expanduser(args.proj_root)).resolve()\n    wroot = (proj / args.weights_root).resolve()\n    out_root = (proj / args.out_subdir).resolve()\n    out_root.mkdir(parents=True, exist_ok=True)\n\n    sweep_csv_path = (proj / args.sweep_csv).resolve() if args.sweep_csv else None\n\n    pts = sorted(list(wroot.rglob(args.pattern)))\n    if not pts:\n        print(f\"No weights found under {wroot} matching {args.pattern}\")\n        return\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Device:\", device)\n\n    # Dataset & classes\n    test_root = (proj / args.test_dir).resolve()\n    test_base = datasets.ImageFolder(str(test_root))\n    classes = test_base.classes\n    num_classes = len(classes)\n    print(\"Classes:\", classes)\n\n    results = []\n\n    for pt in pts:\n        try:\n            arch, params_blob = infer_arch_and_params(pt)\n            if not arch:\n                print(f\"[SKIP] Could not infer architecture for {pt.name}\")\n                continue\n\n            tf_eval, size = make_eval_transform(arch)\n            test_ds = datasets.ImageFolder(str(test_root), transform=tf_eval)\n            test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False,\n                                     num_workers=args.num_workers, pin_memory=True)\n\n            # Build model\n            model, head_name = base_build_model(arch, num_classes)\n            model = model.to(device)\n\n            # Load state (unwrap if nested)\n            state = torch.load(pt, map_location=\"cpu\")\n            if isinstance(state, dict) and \"state_dict\" in state:\n                state = state[\"state_dict\"]\n\n            # Decide dropout for potential head reconstruction\n            dp = pick_dropout_for_arch(arch, params_blob, sweep_csv_path)\n\n            # If saved state expects Sequential head and model has Linear → add dropout head\n            if isinstance(getattr(model, head_name), nn.Linear) and state_uses_sequential_head(state, head_name):\n                if dp is None or dp <= 0:\n                    dp = 0.3  # safe default\n                model = add_head_dropout(model, head_name, dp, num_classes)\n                model = model.to(device)  # ensure rebuilt head is on GPU\n\n            # Try strict load first\n            strict_loaded = try_strict_load(model, state)\n\n            if not strict_loaded:\n                if args.allow_partial_load:\n                    partial_load_state(model, state)\n                else:\n                    # one last attempt: maybe head wasn't reconstructed; try now if Linear\n                    if isinstance(getattr(model, head_name), nn.Linear):\n                        if dp is None or dp <= 0:\n                            dp = 0.3\n                        model = add_head_dropout(model, head_name, dp, num_classes)\n                        model = model.to(device)\n                        strict_loaded = try_strict_load(model, state)\n                    if not strict_loaded:\n                        raise RuntimeError(\"Strict load failed and --allow_partial_load=0.\")\n\n            # Safety: ensure head params are on correct device\n            head_mod = getattr(model, head_name)\n            for p in head_mod.parameters():\n                if p.device != device:\n                    head_mod.to(device)\n                    break\n\n            # Evaluate\n            test_loss, test_acc = evaluate(model, test_loader, device, arch)\n            print(f\"[{arch}] {pt.name}  acc={test_acc:.4f}  StrictLoad={strict_loaded}\")\n\n            # Save per-model short summary (optional but handy)\n            out_dir = out_root / arch\n            out_dir.mkdir(parents=True, exist_ok=True)\n            (out_dir / \"summary.txt\").write_text(\n                f\"Test Acc: {test_acc:.6f}\\n\"\n                f\"Loss: {test_loss:.6f}\\n\"\n                f\"Weights: {pt}\\n\"\n                f\"ImageSize: {size}\\n\"\n                f\"Head: {head_name} | dropout_in_head={'yes' if isinstance(head_mod, nn.Sequential) else 'no'} | dropout_p={dp if dp is not None else 'NA'}\\n\"\n                f\"StrictLoad: {strict_loaded}\\n\"\n            )\n\n            results.append({\n                \"arch\": arch,\n                \"weights_path\": str(pt),\n                \"test_dir\": str(test_root),\n                \"img_size\": size,\n                \"num_classes\": num_classes,\n                \"test_acc\": float(test_acc),\n                \"loss\": float(test_loss),\n                \"out_dir\": str(out_dir),\n                \"strict_loaded\": bool(strict_loaded),\n                \"used_head_dropout\": isinstance(head_mod, nn.Sequential),\n                \"dropout_p\": dp if dp is not None else None,\n            })\n        except KeyboardInterrupt:\n            raise\n        except Exception as e:\n            print(f\"[ERROR] {pt.name}: {e}\")\n\n    # Write aggregate CSV\n    if results:\n        df = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\n        df.to_csv(out_root / \"eval_results.csv\", index=False)\n        print(\"Saved:\", out_root / \"eval_results.csv\")\n    else:\n        print(\"No successful evaluations.\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "main()\n",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
